{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Network Model for Regression.\n",
        "\n",
        "## Assignment-1"
      ],
      "metadata": {
        "id": "tVn2FwxFJkF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Modifying the model in lab 3.2 to do Regression (+5 pts)"
      ],
      "metadata": {
        "id": "EdQuxsiNKS2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Implementing the Learning Algorithm"
      ],
      "metadata": {
        "id": "qvmunKkiKk2d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1 Importing Packages"
      ],
      "metadata": {
        "id": "aJxafeTZK0Ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "metadata": {
        "id": "t-RlHpvTK5A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 Parameter Initialization"
      ],
      "metadata": {
        "id": "WIhXEGh-ODoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "nx is the number of neurons in the input layer (i.e., the number of features in the dataset)\n",
        "nh is the number of neurons in the hidden layer\n",
        "ny is the number of neurons in the output layer (For this example we are using one nueron in the output layer so ny=1)\n",
        "\"\"\"\n",
        "def initialize_parameters(nx,nh,ny):\n",
        "    #set the random seed so the same random values are generated every time you run this function\n",
        "    np.random.seed(1)\n",
        "\n",
        "\n",
        "    #initialize weights to small random numbers and biases to zeros for each layer\n",
        "    W1=np.random.uniform(size=(nh,nx), low=-0.01, high=0.01)\n",
        "    b1=np.zeros((nh,1))\n",
        "    W2=np.random.uniform(size=(ny,nh), low=-0.01, high=0.01)\n",
        "    b2=np.zeros((ny,1))\n",
        "\n",
        "    #create a dictionary of network parameters\n",
        "    parameters = {\"W1\": W1,\n",
        "                  \"b1\": b1,\n",
        "                  \"W2\": W2,\n",
        "                  \"b2\": b2}\n",
        "\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "k5MjouD4OGpb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3 Forward Pass"
      ],
      "metadata": {
        "id": "AWgzK3_MOeqZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#relu activation\n",
        "def relu(z):\n",
        "    return np.maximum(0,z)"
      ],
      "metadata": {
        "id": "lUo3zgeBOjie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "In forward pass we do the computations in the computational graph. We cache the intermediate nodes we will later need in the backward pass\n",
        "\"\"\"\n",
        "def forward_pass(parameters,X):\n",
        "    Z1= np.dot(parameters[\"W1\"],X)+parameters[\"b1\"] # b1 is broadcasted n times before it is added to np.dpt(W1,X1)\n",
        "    A1=relu(Z1)\n",
        "    Z2=np.dot(parameters[\"W2\"],A1)+parameters[\"b2\"] #b2 is broadcasted n times before it is added to np.dpt(W2,A1)\n",
        "    Yhat=(Z2)\n",
        "\n",
        "    cache = {\"A1\": A1,\n",
        "             \"Z1\":Z1,\n",
        "             \"Z2\": Z2}\n",
        "    return Yhat,cache"
      ],
      "metadata": {
        "id": "aaa4yrVpO6GA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also write a utility method to compute the loss"
      ],
      "metadata": {
        "id": "uqT97qwrTDFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Mean Squared Error for the loss function (slide 21). The gradient of Mean Squared Error with respect to the network output ð‘Œð‘Œï¿½ for each training example is computed as follows:"
      ],
      "metadata": {
        "id": "5TXCTCKWSucZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "n is the number of examples, y is a vector of actual/observed outputs and yhat is a vector of predicted outputs\n",
        "\"\"\"\n",
        "def compute_loss(Y, Yhat):\n",
        "\n",
        "    n=Y.shape[1]\n",
        "    loss = (1 / n) * (np.sum((Y - Yhat) * (Y - Yhat)))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "nE2iU1N9Sw2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4 Backward Pass"
      ],
      "metadata": {
        "id": "zPHiKOQCPIIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def dMeanSquareLoss(Y,Yhat):\n",
        "    return (Yhat - Y)\n",
        "\n",
        "\n",
        "def drelu(Z):\n",
        "    \"\"\"\n",
        "np.where(condition, x, y) for each element of the array returns x if condition is true otherwise returns y.\n",
        "In this case for each element Z drelu=1 if the element is greater than 0 otherwise drelu=0\n",
        "\"\"\"\n",
        "    drelu=np.where(Z>0, 1.0, 0.0)\n",
        "    return drelu"
      ],
      "metadata": {
        "id": "lix9zySvVIFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_pass(parameters, cache, X, Y, Yhat):\n",
        "    n=X.shape[1]\n",
        "    dZ2=dMeanSquareLoss(Y, Yhat )*1\n",
        "    dW2=(1/n)*np.dot(dZ2,cache[\"A1\"].T)\n",
        "    db2=(1/n)*np.sum(dZ2, axis=1, keepdims=True)\n",
        "    dA1=np.dot(parameters[\"W2\"].T,dZ2)\n",
        "    dZ1=dA1*drelu(cache[\"Z1\"])\n",
        "    dW1=(1/n)*np.dot(dZ1,X.T)\n",
        "    db1=(1/n)*np.sum(dZ1, axis=1, keepdims=True)\n",
        "    gradients={\"dW1\": dW1,\n",
        "             \"db1\": db1,\n",
        "             \"dW2\":dW2,\n",
        "              \"db2\":db2\n",
        "              }\n",
        "    return gradients"
      ],
      "metadata": {
        "id": "kqqUhyDOXg8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.4 Using Gradient Descent To update the parameters"
      ],
      "metadata": {
        "id": "td6cMaEcX6Vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def update_parameters(parameters, gradients, learning_rate):\n",
        "    parameters[\"W1\"]=parameters[\"W1\"]-learning_rate*gradients[\"dW1\"]\n",
        "    parameters[\"W2\"]=parameters[\"W2\"]-learning_rate*gradients[\"dW2\"]\n",
        "    parameters[\"b1\"]=parameters[\"b1\"]-learning_rate*gradients[\"db1\"]\n",
        "    parameters[\"b2\"]=parameters[\"b2\"]-learning_rate*gradients[\"db2\"]\n",
        "    return parameters"
      ],
      "metadata": {
        "id": "i-CzkpAQX-OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.5 Putting it all together, Creating the NN Model"
      ],
      "metadata": {
        "id": "nCa4GdNVYRBO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Arguments: train_X: is the training dataset (features)\n",
        "           train_Y: is the vector of labels for training_X\n",
        "           val_X: is the vector of validation dataset (features)\n",
        "           val_y: is the vector of labels for val_X\n",
        "           nh: is the number of neurons in the hidden layer\n",
        "           num_iterations: The number of iterations of gradient descent\n",
        "\"\"\"\n",
        "def create_nn_model(train_X,train_Y,nh, val_X, val_Y, num_iterations, learning_rate):\n",
        "    \"\"\"\n",
        "    Do some safety check on the data before proceeding.\n",
        "    train_X and val_X must have the same number of features (i.e., same number of rows)\n",
        "    train_X must have the same number of examples as train_Y (i.e., same number of columns )\n",
        "    val_X must have the same number of examples as Val_Y\n",
        "    \"\"\"\n",
        "    assert(train_X.shape[0]==val_X.shape[0]), \"train_X and val_X must have the same number of features\"\n",
        "    assert(train_X.shape[1]==train_Y.size), \"train_X and train_Y must have the same number of examples\"\n",
        "    assert(val_X.shape[1]==val_Y.size), \"val_X and val_Y must have the same number of examples\"\n",
        "\n",
        "\n",
        "    #getting the number of features\n",
        "    nx=train_X.shape[0]\n",
        "\n",
        "     # We want to use this network for binary classification, so we have only one neuron in the output layer with a sigmoid activation\n",
        "    ny=1\n",
        "\n",
        "    # initializing the parameteres\n",
        "    parameters=initialize_parameters(nx,nh,ny)\n",
        "\n",
        "\n",
        "    #initialize lists to store the training and valideation losses for each iteration.\n",
        "    val_loss=[]\n",
        "    train_loss=[]\n",
        "\n",
        "    #run num_iterations of gradient descent\n",
        "    for i in range (0, num_iterations):\n",
        "        #run the forward pass on train_X\n",
        "        Yhat_train, train_cache= forward_pass(parameters,train_X)\n",
        "\n",
        "        #run the forward pass on val_X\n",
        "        Yhat_val,val_cache= forward_pass(parameters,val_X)\n",
        "\n",
        "        #compute the loss on the train and val datasets\n",
        "        train_loss.append(compute_loss(train_Y,Yhat_train))\n",
        "        val_loss.append(compute_loss(val_Y,Yhat_val))\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        run the backward pass. Note that the backward pass is only run on the training data not the validation data\n",
        "        Because the learning must be only done on the training data and hence, validation data is not used to update\n",
        "        the model parameters.\n",
        "        \"\"\"\n",
        "        gradients=backward_pass(parameters, train_cache, train_X, train_Y,Yhat_train)\n",
        "\n",
        "\n",
        "        # update the parameters\n",
        "        parameters=update_parameters(parameters, gradients, learning_rate)\n",
        "\n",
        "        #print the trianing loss and validation loss for each iteration.\n",
        "        print(\"iteration {} :train_loss:{} val_loss{}\".format(i,train_loss[i],val_loss[i]))\n",
        "\n",
        "    #create a dictionary history and put train_loss and validaiton_loss in it\n",
        "    history={\"val_loss\": val_loss,\n",
        "             \"train_loss\": train_loss}\n",
        "\n",
        "\n",
        "        #return the parameters and the history\n",
        "    return parameters, history"
      ],
      "metadata": {
        "id": "P7X7olyIYUqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.5 predicting and evaluating the NN model"
      ],
      "metadata": {
        "id": "uzFlNPlbYvUL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(parameters,X, prob_threshold=0.5):\n",
        "    Yhat,cache=forward_pass(parameters, X)\n",
        "    # predict class 1 if the output is greater than prob_threshold; otherwise, predict zero\n",
        "    #predicted_label=np.where(Yhat>prob_threshold, 1, 0)\n",
        "    return Yhat"
      ],
      "metadata": {
        "id": "tEBXjAPlRBgp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Preparing California Housing Data (+6pts)**"
      ],
      "metadata": {
        "id": "rJbvRbvbZN7t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "eVqrd8Iy14Zo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cal_df1=pd.read_csv(\"sample_data/california_housing_train.csv\", header=0)\n",
        "\n",
        "cal_df2=pd.read_csv(\"sample_data/california_housing_test.csv\")"
      ],
      "metadata": {
        "id": "6XsVyrVVZbUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Split the training data into 80% training and 20% validation. There are several ways to do this; for instance, you can use dataframe sample method"
      ],
      "metadata": {
        "id": "MQS0cVbj1-cI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_cal1 = cal_df1.sample(frac=0.8)\n",
        "val_cal1 = cal_df1.drop(train_cal1.index)"
      ],
      "metadata": {
        "id": "xtv6HrNo2GZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the train/validation/and test data into numpy arrays using to_numpy method"
      ],
      "metadata": {
        "id": "5_y8C4kf2kvw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_cal_2 = train_cal1.to_numpy()\n",
        "val_cal_2 = val_cal1.to_numpy()\n",
        "\n",
        "train_cal = np.transpose (train_cal_2)\n",
        "val_cal = np.transpose(val_cal_2)\n",
        "\n",
        "test_cal1 = cal_df2.to_numpy()\n",
        "test_cal = np.transpose(test_cal1)"
      ],
      "metadata": {
        "id": "KjjxUReV2q-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_cal.shape)\n",
        "print(val_cal.shape)\n",
        "print(test_cal.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vrt2ml-4xXt",
        "outputId": "30fd6de4-c914-4fdb-aad8-ede3e41f523e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(9, 13600)\n",
            "(9, 3400)\n",
            "(9, 3000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#everything minus the last row is X\n",
        "train_cal_X=train_cal[:-1,]\n",
        "#the last row (at index -1) is Y\n",
        "train_cal_Y=train_cal[-1,:]\n",
        "\n",
        "# the labels train_Y and val_Y have to be reshaped to a 2D array for the matrix operations to work in the forward and backward passes\n",
        "train_cal_Y=np.reshape(train_cal_Y, (1,train_cal_Y.size))\n",
        "\n",
        "\n",
        "#The mean function calculates the mean of the elements along a given axis (in this case axis=1 indicates calculation along the rows) and the keepdims argument ensures that the mean is returned as a 2D array with a single column,\n",
        "#even if the input array is 1D. The std function calculates the standard deviation in the same way.\n",
        "mean_cal_train = train_cal_X.mean(axis=1, keepdims=True)\n",
        "sdv_cal_train = train_cal_X.std(axis=1, keepdims=True)\n"
      ],
      "metadata": {
        "id": "Gy4XBwBW6pbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_cal_X=val_cal[:-1,]\n",
        "val_cal_Y=val_cal[-1,]\n",
        "val_cal_Y=np.reshape(val_cal_Y, (1,val_cal_Y.size))\n",
        "\n",
        "test_cal_X=test_cal[:-1,]\n",
        "test_cal_Y=test_cal[-1,]\n",
        "test_cal_Y=np.reshape(test_cal_Y, (1,test_cal_Y.size))"
      ],
      "metadata": {
        "id": "jTyvkFe4-WZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "subtracting the mean and normalizing the data,By dividing the features with their standard deviation, the features are scaled to unit variance."
      ],
      "metadata": {
        "id": "aVNimFN7_lN_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "traincalX = train_cal_X - mean_cal_train\n",
        "traincalX = traincalX/sdv_cal_train\n",
        "\n",
        "valcalX = val_cal_X - mean_cal_train\n",
        "valcalX = valcalX/sdv_cal_train\n",
        "\n",
        "testcalX = test_cal_X - mean_cal_train\n",
        "testcalX = testcalX/sdv_cal_train"
      ],
      "metadata": {
        "id": "M1bVlVQf_Dmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Letâ€™s divide the median_house_values by 100K to scale them down."
      ],
      "metadata": {
        "id": "wwe3W6Hw_8pv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "traincalY = train_cal_Y/100000\n",
        "valcalY = val_cal_Y/100000\n",
        "testcalY = test_cal_Y/100000"
      ],
      "metadata": {
        "id": "KSXaAutb_zno"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_X = traincalX\n",
        "val_X = valcalX\n",
        "test_X = testcalX\n",
        "\n",
        "train_Y = traincalY\n",
        "val_Y = valcalY\n",
        "test_Y = testcalY"
      ],
      "metadata": {
        "id": "fJfCUpg0BHME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_X.shape)\n",
        "print(val_X.shape)\n",
        "print(test_X.shape)\n",
        "print(train_Y.shape)\n",
        "print(val_Y.shape)\n",
        "print(val_Y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVCjzKYHBWjX",
        "outputId": "3f938de3-8b24-49fe-a283-798e8bcf3ee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8, 13600)\n",
            "(8, 3400)\n",
            "(8, 3000)\n",
            "(1, 13600)\n",
            "(1, 3400)\n",
            "(1, 3400)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "iterations=1000\n",
        "parameters, history=create_nn_model(train_X,train_Y,60, val_X,val_Y, iterations,0.08)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1B0TXg2BFcfV",
        "outputId": "53d1428b-9ffa-49bf-dd6b-a66e81c2ed21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iteration 0 :train_loss:5.642663893564969 val_loss5.642304540838271\n",
            "iteration 1 :train_loss:4.980604192886688 val_loss4.978952975515447\n",
            "iteration 2 :train_loss:4.420455267162027 val_loss4.417619954187638\n",
            "iteration 3 :train_loss:3.9463933895806487 val_loss3.9424742243013986\n",
            "iteration 4 :train_loss:3.5450900222673796 val_loss3.5401801236546064\n",
            "iteration 5 :train_loss:3.2052982945705115 val_loss3.199484193822386\n",
            "iteration 6 :train_loss:2.9175174026107675 val_loss2.9108799964940513\n",
            "iteration 7 :train_loss:2.6737147848602767 val_loss2.6663306090257364\n",
            "iteration 8 :train_loss:2.467096542324925 val_loss2.4590363438560265\n",
            "iteration 9 :train_loss:2.291910067919766 val_loss2.28324126636165\n",
            "iteration 10 :train_loss:2.1432858065371736 val_loss2.1340726578226232\n",
            "iteration 11 :train_loss:2.0170974158040806 val_loss2.007403362593241\n",
            "iteration 12 :train_loss:1.9098471264099224 val_loss1.899733485781869\n",
            "iteration 13 :train_loss:1.8185696448889925 val_loss1.808096326516181\n",
            "iteration 14 :train_loss:1.7407512301668946 val_loss1.7299777048992402\n",
            "iteration 15 :train_loss:1.6742564529870232 val_loss1.6632408389232107\n",
            "iteration 16 :train_loss:1.6172704120320962 val_loss1.6060690604284558\n",
            "iteration 17 :train_loss:1.568247765969049 val_loss1.5569171481583055\n",
            "iteration 18 :train_loss:1.5258774771164323 val_loss1.514473473300888\n",
            "iteration 19 :train_loss:1.4890420339134975 val_loss1.477623416058876\n",
            "iteration 20 :train_loss:1.4567862105837777 val_loss1.4454134913062435\n",
            "iteration 21 :train_loss:1.4282975524790413 val_loss1.4170314605991499\n",
            "iteration 22 :train_loss:1.4028796549321376 val_loss1.3917802939110628\n",
            "iteration 23 :train_loss:1.3799398442645487 val_loss1.3690707219624507\n",
            "iteration 24 :train_loss:1.358971461169804 val_loss1.348397514160981\n",
            "iteration 25 :train_loss:1.3395384197467337 val_loss1.329325855947739\n",
            "iteration 26 :train_loss:1.321270398629011 val_loss1.3114888173617045\n",
            "iteration 27 :train_loss:1.3038459257399255 val_loss1.2945665588725022\n",
            "iteration 28 :train_loss:1.286993805873747 val_loss1.2782907286165586\n",
            "iteration 29 :train_loss:1.2704817178868735 val_loss1.2624307602973774\n",
            "iteration 30 :train_loss:1.2541130841740344 val_loss1.246791473013373\n",
            "iteration 31 :train_loss:1.2377279279029292 val_loss1.2312135214604942\n",
            "iteration 32 :train_loss:1.22119453361478 val_loss1.2155661039649925\n",
            "iteration 33 :train_loss:1.2044095400027393 val_loss1.19974511512756\n",
            "iteration 34 :train_loss:1.1872938684673415 val_loss1.1836776734977803\n",
            "iteration 35 :train_loss:1.1697942566011788 val_loss1.1673098708228014\n",
            "iteration 36 :train_loss:1.1518825468343015 val_loss1.1506071698467026\n",
            "iteration 37 :train_loss:1.1335532818305167 val_loss1.133560787346743\n",
            "iteration 38 :train_loss:1.1148248044408395 val_loss1.1161826964571162\n",
            "iteration 39 :train_loss:1.095731079800829 val_loss1.0985031095565156\n",
            "iteration 40 :train_loss:1.0763229141037336 val_loss1.080572667976644\n",
            "iteration 41 :train_loss:1.0566726940695945 val_loss1.0624510426520377\n",
            "iteration 42 :train_loss:1.0368619832609434 val_loss1.0442169091810634\n",
            "iteration 43 :train_loss:1.0169773296454958 val_loss1.025948325768344\n",
            "iteration 44 :train_loss:0.9971212475238825 val_loss1.0077368475923958\n",
            "iteration 45 :train_loss:0.9773960495951614 val_loss0.9896784990945642\n",
            "iteration 46 :train_loss:0.9579082943193249 val_loss0.9718618734381906\n",
            "iteration 47 :train_loss:0.938761210696144 val_loss0.9543813918114319\n",
            "iteration 48 :train_loss:0.9200468176064936 val_loss0.9373151096126026\n",
            "iteration 49 :train_loss:0.9018536860019933 val_loss0.9207442051065842\n",
            "iteration 50 :train_loss:0.8842566308920384 val_loss0.9047374416220555\n",
            "iteration 51 :train_loss:0.8673148455718256 val_loss0.8893476414912076\n",
            "iteration 52 :train_loss:0.8510766939063379 val_loss0.8746161633861596\n",
            "iteration 53 :train_loss:0.8355760061938005 val_loss0.8605732757702681\n",
            "iteration 54 :train_loss:0.8208342272020805 val_loss0.8472353552583344\n",
            "iteration 55 :train_loss:0.8068619141242167 val_loss0.8346062832496537\n",
            "iteration 56 :train_loss:0.7936582027269674 val_loss0.8226830495096937\n",
            "iteration 57 :train_loss:0.7812111524197978 val_loss0.8114510148325937\n",
            "iteration 58 :train_loss:0.7695037588269117 val_loss0.800890700240267\n",
            "iteration 59 :train_loss:0.7585074694812839 val_loss0.7909772130757188\n",
            "iteration 60 :train_loss:0.7481931486874303 val_loss0.7816791887513446\n",
            "iteration 61 :train_loss:0.7385269654616612 val_loss0.7729642099714003\n",
            "iteration 62 :train_loss:0.7294730680694893 val_loss0.764801000651754\n",
            "iteration 63 :train_loss:0.7209931698791641 val_loss0.7571509299703582\n",
            "iteration 64 :train_loss:0.7130496327174528 val_loss0.7499798010885232\n",
            "iteration 65 :train_loss:0.7056053521361241 val_loss0.743254076107056\n",
            "iteration 66 :train_loss:0.698625197235251 val_loss0.7369419504722559\n",
            "iteration 67 :train_loss:0.6920772106332139 val_loss0.7310141292612486\n",
            "iteration 68 :train_loss:0.6859297568911361 val_loss0.7254420935734227\n",
            "iteration 69 :train_loss:0.6801518294512668 val_loss0.7201973037504716\n",
            "iteration 70 :train_loss:0.6747171198426182 val_loss0.7152541100261809\n",
            "iteration 71 :train_loss:0.6695982949359349 val_loss0.710588710716433\n",
            "iteration 72 :train_loss:0.6647711691689494 val_loss0.7061794509720843\n",
            "iteration 73 :train_loss:0.6602121087619405 val_loss0.702004989035383\n",
            "iteration 74 :train_loss:0.6559024582406177 val_loss0.6980479560437913\n",
            "iteration 75 :train_loss:0.6518224448801402 val_loss0.6942898409403147\n",
            "iteration 76 :train_loss:0.6479537224277909 val_loss0.6907163104237394\n",
            "iteration 77 :train_loss:0.6442777710382099 val_loss0.6873082800988215\n",
            "iteration 78 :train_loss:0.6407809849491575 val_loss0.6840540846404425\n",
            "iteration 79 :train_loss:0.6374493611604152 val_loss0.6809423743762892\n",
            "iteration 80 :train_loss:0.6342703135724667 val_loss0.6779609189898327\n",
            "iteration 81 :train_loss:0.6312302699676149 val_loss0.675099262326926\n",
            "iteration 82 :train_loss:0.628318568762735 val_loss0.6723474081286135\n",
            "iteration 83 :train_loss:0.6255246444774317 val_loss0.6696985504324873\n",
            "iteration 84 :train_loss:0.6228393616014414 val_loss0.6671417093089664\n",
            "iteration 85 :train_loss:0.6202554143576688 val_loss0.6646683444000616\n",
            "iteration 86 :train_loss:0.6177639738763986 val_loss0.6622685812967349\n",
            "iteration 87 :train_loss:0.6153581659444264 val_loss0.6599381009995814\n",
            "iteration 88 :train_loss:0.6130322163056245 val_loss0.6576755949244454\n",
            "iteration 89 :train_loss:0.6107816843207723 val_loss0.6554756337685494\n",
            "iteration 90 :train_loss:0.6086004773068856 val_loss0.6533296268392755\n",
            "iteration 91 :train_loss:0.6064839652362086 val_loss0.6512350252650705\n",
            "iteration 92 :train_loss:0.604427572046471 val_loss0.6491892611492296\n",
            "iteration 93 :train_loss:0.6024247344485123 val_loss0.647187973563196\n",
            "iteration 94 :train_loss:0.6004714699617564 val_loss0.645227062640088\n",
            "iteration 95 :train_loss:0.5985645924805837 val_loss0.6433012494811207\n",
            "iteration 96 :train_loss:0.5967034293919723 val_loss0.6414137991467028\n",
            "iteration 97 :train_loss:0.5948849310937651 val_loss0.639561848513524\n",
            "iteration 98 :train_loss:0.5931067785785605 val_loss0.6377455701973591\n",
            "iteration 99 :train_loss:0.5913638706935594 val_loss0.6359591978962367\n",
            "iteration 100 :train_loss:0.5896528420014153 val_loss0.6341958292493172\n",
            "iteration 101 :train_loss:0.5879731018713 val_loss0.6324581930285077\n",
            "iteration 102 :train_loss:0.5863229443931453 val_loss0.6307440999918464\n",
            "iteration 103 :train_loss:0.5847010531209932 val_loss0.6290552897523723\n",
            "iteration 104 :train_loss:0.5831056379789982 val_loss0.6273874219119727\n",
            "iteration 105 :train_loss:0.5815339788419782 val_loss0.6257373932058707\n",
            "iteration 106 :train_loss:0.5799824145358443 val_loss0.624104850372604\n",
            "iteration 107 :train_loss:0.5784521203688293 val_loss0.6224911737944846\n",
            "iteration 108 :train_loss:0.5769418616774222 val_loss0.6208944959741269\n",
            "iteration 109 :train_loss:0.5754505134951833 val_loss0.6193112779185067\n",
            "iteration 110 :train_loss:0.5739779255527707 val_loss0.6177396149712134\n",
            "iteration 111 :train_loss:0.572523317826988 val_loss0.6161815873196709\n",
            "iteration 112 :train_loss:0.5710844392780271 val_loss0.6146363599077834\n",
            "iteration 113 :train_loss:0.5696605499115454 val_loss0.6131029162013842\n",
            "iteration 114 :train_loss:0.5682538705721963 val_loss0.6115820123764807\n",
            "iteration 115 :train_loss:0.5668608129604807 val_loss0.6100727371851647\n",
            "iteration 116 :train_loss:0.5654811016095864 val_loss0.6085737331038283\n",
            "iteration 117 :train_loss:0.5641164446977279 val_loss0.607087490357352\n",
            "iteration 118 :train_loss:0.5627639460307564 val_loss0.6056087888946231\n",
            "iteration 119 :train_loss:0.5614227883648509 val_loss0.6041403445958958\n",
            "iteration 120 :train_loss:0.5600916984011745 val_loss0.6026788111334709\n",
            "iteration 121 :train_loss:0.5587727101938506 val_loss0.6012265405453017\n",
            "iteration 122 :train_loss:0.5574669412003758 val_loss0.5997855288245564\n",
            "iteration 123 :train_loss:0.5561736370806661 val_loss0.5983575867298565\n",
            "iteration 124 :train_loss:0.5548893938896937 val_loss0.596938684650173\n",
            "iteration 125 :train_loss:0.5536152803364566 val_loss0.5955267157430794\n",
            "iteration 126 :train_loss:0.5523516459547989 val_loss0.5941244434938328\n",
            "iteration 127 :train_loss:0.5510995583565523 val_loss0.5927344703035357\n",
            "iteration 128 :train_loss:0.5498561815036658 val_loss0.591353424646499\n",
            "iteration 129 :train_loss:0.5486192114081114 val_loss0.5899795812933073\n",
            "iteration 130 :train_loss:0.547391091556858 val_loss0.5886137226986179\n",
            "iteration 131 :train_loss:0.5461726438543246 val_loss0.5872557130375721\n",
            "iteration 132 :train_loss:0.5449644250506404 val_loss0.5859064234773951\n",
            "iteration 133 :train_loss:0.5437675269922502 val_loss0.5845668664690606\n",
            "iteration 134 :train_loss:0.54258134891332 val_loss0.5832376515931746\n",
            "iteration 135 :train_loss:0.5414044871508186 val_loss0.581916920365829\n",
            "iteration 136 :train_loss:0.5402379857335365 val_loss0.58060647332113\n",
            "iteration 137 :train_loss:0.539081779591089 val_loss0.5793036634471775\n",
            "iteration 138 :train_loss:0.5379365357209118 val_loss0.5780134203184848\n",
            "iteration 139 :train_loss:0.536799660689288 val_loss0.5767315554999524\n",
            "iteration 140 :train_loss:0.5356723174568213 val_loss0.5754597128711778\n",
            "iteration 141 :train_loss:0.534554244469944 val_loss0.5741982465230568\n",
            "iteration 142 :train_loss:0.5334454305771068 val_loss0.5729455058471739\n",
            "iteration 143 :train_loss:0.5323470491388181 val_loss0.5717030196660978\n",
            "iteration 144 :train_loss:0.5312576742921906 val_loss0.570468563301762\n",
            "iteration 145 :train_loss:0.5301770117961832 val_loss0.5692407063580942\n",
            "iteration 146 :train_loss:0.529105647742852 val_loss0.5680239261626552\n",
            "iteration 147 :train_loss:0.5280418673617657 val_loss0.5668146155312686\n",
            "iteration 148 :train_loss:0.5269891964179908 val_loss0.565613998897901\n",
            "iteration 149 :train_loss:0.5259468015424978 val_loss0.5644248005263065\n",
            "iteration 150 :train_loss:0.5249148472858186 val_loss0.5632455083960084\n",
            "iteration 151 :train_loss:0.5238913629529566 val_loss0.5620738407396476\n",
            "iteration 152 :train_loss:0.5228768699135765 val_loss0.5609112974461736\n",
            "iteration 153 :train_loss:0.5218728059022809 val_loss0.5597585235687651\n",
            "iteration 154 :train_loss:0.520878018709686 val_loss0.5586130326402416\n",
            "iteration 155 :train_loss:0.5198933207547898 val_loss0.5574791041525904\n",
            "iteration 156 :train_loss:0.5189157525251308 val_loss0.556353908400465\n",
            "iteration 157 :train_loss:0.5179458624150259 val_loss0.5552360830131385\n",
            "iteration 158 :train_loss:0.5169851045516277 val_loss0.5541254453039137\n",
            "iteration 159 :train_loss:0.5160339582118584 val_loss0.5530237529982339\n",
            "iteration 160 :train_loss:0.5150912170590751 val_loss0.5519291712244334\n",
            "iteration 161 :train_loss:0.5141570533150641 val_loss0.5508407605207009\n",
            "iteration 162 :train_loss:0.5132325917965348 val_loss0.54976393359449\n",
            "iteration 163 :train_loss:0.5123179403656345 val_loss0.5486964063458344\n",
            "iteration 164 :train_loss:0.5114126610353769 val_loss0.5476381645502715\n",
            "iteration 165 :train_loss:0.5105140883442187 val_loss0.5465865206883218\n",
            "iteration 166 :train_loss:0.5096243422710764 val_loss0.5455421732289776\n",
            "iteration 167 :train_loss:0.508743691720787 val_loss0.5445059943253108\n",
            "iteration 168 :train_loss:0.5078709216189359 val_loss0.5434791902542125\n",
            "iteration 169 :train_loss:0.5070059018553873 val_loss0.542460491590058\n",
            "iteration 170 :train_loss:0.5061489283931098 val_loss0.5414471193704211\n",
            "iteration 171 :train_loss:0.5053011473351754 val_loss0.540443711039732\n",
            "iteration 172 :train_loss:0.5044625654982811 val_loss0.5394492539966785\n",
            "iteration 173 :train_loss:0.5036340413495071 val_loss0.538465025624987\n",
            "iteration 174 :train_loss:0.5028151256105496 val_loss0.5374888603359158\n",
            "iteration 175 :train_loss:0.5020048651722568 val_loss0.5365222852249676\n",
            "iteration 176 :train_loss:0.5012034387492612 val_loss0.5355658700241233\n",
            "iteration 177 :train_loss:0.5004097598777755 val_loss0.5346185788450513\n",
            "iteration 178 :train_loss:0.4996244123760964 val_loss0.5336798561637115\n",
            "iteration 179 :train_loss:0.4988472902884429 val_loss0.532750056935489\n",
            "iteration 180 :train_loss:0.49807913710942514 val_loss0.5318289487525559\n",
            "iteration 181 :train_loss:0.4973197367865821 val_loss0.5309175122144141\n",
            "iteration 182 :train_loss:0.4965696632845301 val_loss0.5300146419167374\n",
            "iteration 183 :train_loss:0.4958283302122327 val_loss0.5291219044679174\n",
            "iteration 184 :train_loss:0.4950947723352265 val_loss0.5282375445930494\n",
            "iteration 185 :train_loss:0.49436910422715 val_loss0.5273626957486457\n",
            "iteration 186 :train_loss:0.4936508164333669 val_loss0.5264945297021945\n",
            "iteration 187 :train_loss:0.49294083115971166 val_loss0.5256358292172296\n",
            "iteration 188 :train_loss:0.4922387038336869 val_loss0.5247866933959306\n",
            "iteration 189 :train_loss:0.4915448616868997 val_loss0.5239478161112081\n",
            "iteration 190 :train_loss:0.4908586497473675 val_loss0.5231182034761045\n",
            "iteration 191 :train_loss:0.4901797457557425 val_loss0.522295265313306\n",
            "iteration 192 :train_loss:0.4895078039535254 val_loss0.5214818815942462\n",
            "iteration 193 :train_loss:0.4888437101567569 val_loss0.5206767723652259\n",
            "iteration 194 :train_loss:0.4881863796526387 val_loss0.5198800564466239\n",
            "iteration 195 :train_loss:0.4875366511682785 val_loss0.5190922371404649\n",
            "iteration 196 :train_loss:0.4868929863820642 val_loss0.5183132430504728\n",
            "iteration 197 :train_loss:0.486256512479959 val_loss0.5175424607636631\n",
            "iteration 198 :train_loss:0.4856277054320214 val_loss0.5167797007386595\n",
            "iteration 199 :train_loss:0.48500658722563394 val_loss0.5160255134573564\n",
            "iteration 200 :train_loss:0.48439224430375927 val_loss0.5152794266381678\n",
            "iteration 201 :train_loss:0.483784861439429 val_loss0.5145421468310606\n",
            "iteration 202 :train_loss:0.4831848127448094 val_loss0.5138124006082057\n",
            "iteration 203 :train_loss:0.48259229751099164 val_loss0.5130915503178706\n",
            "iteration 204 :train_loss:0.4820065594351795 val_loss0.5123792797432049\n",
            "iteration 205 :train_loss:0.48142675105771054 val_loss0.5116740269910512\n",
            "iteration 206 :train_loss:0.4808540348604896 val_loss0.5109773241162244\n",
            "iteration 207 :train_loss:0.48028785849688194 val_loss0.5102879708610356\n",
            "iteration 208 :train_loss:0.479726894884674 val_loss0.5096057787201355\n",
            "iteration 209 :train_loss:0.4791704374391078 val_loss0.5089326304115267\n",
            "iteration 210 :train_loss:0.4786197361562225 val_loss0.5082650887207874\n",
            "iteration 211 :train_loss:0.47807490318697005 val_loss0.5076042482136907\n",
            "iteration 212 :train_loss:0.47753494146742176 val_loss0.5069505948914719\n",
            "iteration 213 :train_loss:0.4769998540498175 val_loss0.5063052410532188\n",
            "iteration 214 :train_loss:0.47647082149127956 val_loss0.5056660257285791\n",
            "iteration 215 :train_loss:0.47594734186174004 val_loss0.5050328500383043\n",
            "iteration 216 :train_loss:0.47542960002897544 val_loss0.5044070164704944\n",
            "iteration 217 :train_loss:0.4749178407670312 val_loss0.5037895243793465\n",
            "iteration 218 :train_loss:0.47441136984447185 val_loss0.5031782176622588\n",
            "iteration 219 :train_loss:0.473909916279905 val_loss0.5025734946740459\n",
            "iteration 220 :train_loss:0.4734135833865802 val_loss0.5019745303541443\n",
            "iteration 221 :train_loss:0.4729227255167282 val_loss0.5013828053583168\n",
            "iteration 222 :train_loss:0.4724371871548282 val_loss0.5007972055766489\n",
            "iteration 223 :train_loss:0.4719569205534734 val_loss0.5002190721618895\n",
            "iteration 224 :train_loss:0.47148152161980544 val_loss0.49964693911275\n",
            "iteration 225 :train_loss:0.47101128586957314 val_loss0.49908001396855817\n",
            "iteration 226 :train_loss:0.47054561686810686 val_loss0.498519549273857\n",
            "iteration 227 :train_loss:0.4700857420445353 val_loss0.4979655921280596\n",
            "iteration 228 :train_loss:0.4696303822177229 val_loss0.49741910644617476\n",
            "iteration 229 :train_loss:0.4691812585682966 val_loss0.49687910058485607\n",
            "iteration 230 :train_loss:0.4687380377687212 val_loss0.49634549700497255\n",
            "iteration 231 :train_loss:0.46829938627519657 val_loss0.49581747614982674\n",
            "iteration 232 :train_loss:0.4678651387069253 val_loss0.4952939709222349\n",
            "iteration 233 :train_loss:0.46743564692971384 val_loss0.4947757158445887\n",
            "iteration 234 :train_loss:0.46701109519544176 val_loss0.4942650009267074\n",
            "iteration 235 :train_loss:0.4665913541227617 val_loss0.4937603629674898\n",
            "iteration 236 :train_loss:0.46617659708451004 val_loss0.49326109080007247\n",
            "iteration 237 :train_loss:0.46576581910437637 val_loss0.4927667887058504\n",
            "iteration 238 :train_loss:0.4653598233096312 val_loss0.4922782059406523\n",
            "iteration 239 :train_loss:0.46495822889879873 val_loss0.49179449313709306\n",
            "iteration 240 :train_loss:0.464561302885515 val_loss0.49131749585931445\n",
            "iteration 241 :train_loss:0.4641694942568808 val_loss0.49084679928814534\n",
            "iteration 242 :train_loss:0.46378225195474293 val_loss0.490382644284584\n",
            "iteration 243 :train_loss:0.4633992353870196 val_loss0.48992375287777573\n",
            "iteration 244 :train_loss:0.46302080801259465 val_loss0.4894700140229986\n",
            "iteration 245 :train_loss:0.46264681410965464 val_loss0.4890224298216052\n",
            "iteration 246 :train_loss:0.4622768456647429 val_loss0.48858001121713024\n",
            "iteration 247 :train_loss:0.4619111394879607 val_loss0.4881437944424914\n",
            "iteration 248 :train_loss:0.4615495610721775 val_loss0.4877135672228941\n",
            "iteration 249 :train_loss:0.46119250686145236 val_loss0.48728841475130413\n",
            "iteration 250 :train_loss:0.4608396228369768 val_loss0.4868674966311337\n",
            "iteration 251 :train_loss:0.46049072363346216 val_loss0.4864526120141114\n",
            "iteration 252 :train_loss:0.4601449210027578 val_loss0.48604122697794805\n",
            "iteration 253 :train_loss:0.45980261294650127 val_loss0.48563408075171494\n",
            "iteration 254 :train_loss:0.45946413075363934 val_loss0.48523147771534586\n",
            "iteration 255 :train_loss:0.4591291558131753 val_loss0.4848331776570727\n",
            "iteration 256 :train_loss:0.4587980661752976 val_loss0.4844397431290247\n",
            "iteration 257 :train_loss:0.458470762069065 val_loss0.4840512768021323\n",
            "iteration 258 :train_loss:0.45814748046010206 val_loss0.4836671367667862\n",
            "iteration 259 :train_loss:0.45782789635728754 val_loss0.48328784718093626\n",
            "iteration 260 :train_loss:0.4575113282042136 val_loss0.4829138741290912\n",
            "iteration 261 :train_loss:0.45719722073642594 val_loss0.4825472996796366\n",
            "iteration 262 :train_loss:0.45688707460439076 val_loss0.4821832904406498\n",
            "iteration 263 :train_loss:0.45658012813074905 val_loss0.4818249053792147\n",
            "iteration 264 :train_loss:0.4562761650192569 val_loss0.4814697717547381\n",
            "iteration 265 :train_loss:0.4559756639771581 val_loss0.48111763041715216\n",
            "iteration 266 :train_loss:0.45567815101373016 val_loss0.48076756824356476\n",
            "iteration 267 :train_loss:0.45538307221966623 val_loss0.4804221314302152\n",
            "iteration 268 :train_loss:0.4550908804501067 val_loss0.4800802322564104\n",
            "iteration 269 :train_loss:0.45480177956719964 val_loss0.4797430634210628\n",
            "iteration 270 :train_loss:0.4545150667670797 val_loss0.47940943133301755\n",
            "iteration 271 :train_loss:0.4542308426863479 val_loss0.479079469366036\n",
            "iteration 272 :train_loss:0.45395083209156056 val_loss0.47875153433499873\n",
            "iteration 273 :train_loss:0.45367379138140346 val_loss0.47842785223738643\n",
            "iteration 274 :train_loss:0.4533992377915243 val_loss0.4781067005119384\n",
            "iteration 275 :train_loss:0.45312749437533306 val_loss0.4777901391970878\n",
            "iteration 276 :train_loss:0.4528594425024879 val_loss0.47747573155974216\n",
            "iteration 277 :train_loss:0.45259440931500533 val_loss0.47716512986519594\n",
            "iteration 278 :train_loss:0.4523325517812558 val_loss0.4768589058813138\n",
            "iteration 279 :train_loss:0.4520735593261339 val_loss0.47655515128485293\n",
            "iteration 280 :train_loss:0.4518162644525864 val_loss0.4762571744672135\n",
            "iteration 281 :train_loss:0.4515614790434582 val_loss0.4759598941398394\n",
            "iteration 282 :train_loss:0.4513092033365177 val_loss0.4756658502449819\n",
            "iteration 283 :train_loss:0.45105850633815964 val_loss0.4753744448864706\n",
            "iteration 284 :train_loss:0.45080846724233 val_loss0.475087026901486\n",
            "iteration 285 :train_loss:0.45056032613409286 val_loss0.47480168197781564\n",
            "iteration 286 :train_loss:0.4503142845695955 val_loss0.4745181984390545\n",
            "iteration 287 :train_loss:0.45006986866268056 val_loss0.4742380230626613\n",
            "iteration 288 :train_loss:0.44982582101708907 val_loss0.47396264162929835\n",
            "iteration 289 :train_loss:0.4495842810457438 val_loss0.47368867225308253\n",
            "iteration 290 :train_loss:0.4493453851503466 val_loss0.4734173043929192\n",
            "iteration 291 :train_loss:0.4491094345992117 val_loss0.4731496935769374\n",
            "iteration 292 :train_loss:0.44887640186825495 val_loss0.4728851251440754\n",
            "iteration 293 :train_loss:0.44864434597279523 val_loss0.472624535752294\n",
            "iteration 294 :train_loss:0.4484139445606766 val_loss0.4723673895499859\n",
            "iteration 295 :train_loss:0.4481846652095775 val_loss0.4721139028174923\n",
            "iteration 296 :train_loss:0.44795729950887253 val_loss0.4718614984195877\n",
            "iteration 297 :train_loss:0.44773109508416775 val_loss0.47161325019221934\n",
            "iteration 298 :train_loss:0.44750654905136633 val_loss0.4713644941240628\n",
            "iteration 299 :train_loss:0.44728413129554256 val_loss0.4711173887819616\n",
            "iteration 300 :train_loss:0.4470641659052602 val_loss0.47087235990306625\n",
            "iteration 301 :train_loss:0.446846601380721 val_loss0.4706301009279586\n",
            "iteration 302 :train_loss:0.4466312614864392 val_loss0.47039088163309084\n",
            "iteration 303 :train_loss:0.4464190291486238 val_loss0.4701540101601244\n",
            "iteration 304 :train_loss:0.44621010977201225 val_loss0.4699180775720535\n",
            "iteration 305 :train_loss:0.44600338001480605 val_loss0.46968613320788466\n",
            "iteration 306 :train_loss:0.44579929757075965 val_loss0.4694573323300257\n",
            "iteration 307 :train_loss:0.4455969864228586 val_loss0.469230819487057\n",
            "iteration 308 :train_loss:0.4453966353815318 val_loss0.4690072501008282\n",
            "iteration 309 :train_loss:0.44519889816027064 val_loss0.4687849899103913\n",
            "iteration 310 :train_loss:0.44500263059700607 val_loss0.46856648709827375\n",
            "iteration 311 :train_loss:0.4448083634595329 val_loss0.46834856057838875\n",
            "iteration 312 :train_loss:0.4446159480354698 val_loss0.46813263367908114\n",
            "iteration 313 :train_loss:0.44442554356857844 val_loss0.4679204419111466\n",
            "iteration 314 :train_loss:0.4442368301059731 val_loss0.4677065610578723\n",
            "iteration 315 :train_loss:0.44404904342256557 val_loss0.4674968432495326\n",
            "iteration 316 :train_loss:0.44386217282676477 val_loss0.4672900433117518\n",
            "iteration 317 :train_loss:0.4436762781587962 val_loss0.4670855511807057\n",
            "iteration 318 :train_loss:0.4434919629838991 val_loss0.46688536200377473\n",
            "iteration 319 :train_loss:0.4433096603466068 val_loss0.46668550580203033\n",
            "iteration 320 :train_loss:0.44312899343801165 val_loss0.46648799562607024\n",
            "iteration 321 :train_loss:0.4429498314226003 val_loss0.46629249101681186\n",
            "iteration 322 :train_loss:0.44277176107265775 val_loss0.4660996299193953\n",
            "iteration 323 :train_loss:0.4425944890336926 val_loss0.4659098912786716\n",
            "iteration 324 :train_loss:0.44241899755762387 val_loss0.46572151852899896\n",
            "iteration 325 :train_loss:0.44224558150156446 val_loss0.46553148942068756\n",
            "iteration 326 :train_loss:0.44207367531475944 val_loss0.465343844641898\n",
            "iteration 327 :train_loss:0.4419034665783418 val_loss0.4651585981465804\n",
            "iteration 328 :train_loss:0.44173479229106455 val_loss0.46497596663996865\n",
            "iteration 329 :train_loss:0.441567704942201 val_loss0.46479540062993824\n",
            "iteration 330 :train_loss:0.44140151905799513 val_loss0.4646168347376928\n",
            "iteration 331 :train_loss:0.4412362393845406 val_loss0.46444188902187616\n",
            "iteration 332 :train_loss:0.44107180000334073 val_loss0.4642697193189985\n",
            "iteration 333 :train_loss:0.440909399120159 val_loss0.46409788155110776\n",
            "iteration 334 :train_loss:0.440748283201948 val_loss0.46392755975687894\n",
            "iteration 335 :train_loss:0.4405884541067444 val_loss0.463757188143817\n",
            "iteration 336 :train_loss:0.4404297534413158 val_loss0.46358946925010736\n",
            "iteration 337 :train_loss:0.4402721357270682 val_loss0.46342391094732704\n",
            "iteration 338 :train_loss:0.44011593872713833 val_loss0.46325902575811206\n",
            "iteration 339 :train_loss:0.4399606201570043 val_loss0.46309633760084046\n",
            "iteration 340 :train_loss:0.43980635355950354 val_loss0.4629341339930923\n",
            "iteration 341 :train_loss:0.4396532615990292 val_loss0.4627730878324619\n",
            "iteration 342 :train_loss:0.4395012720986139 val_loss0.4626131702568378\n",
            "iteration 343 :train_loss:0.4393512482771912 val_loss0.46245464815399717\n",
            "iteration 344 :train_loss:0.43920276480881015 val_loss0.46229641074093075\n",
            "iteration 345 :train_loss:0.4390560192724758 val_loss0.46214080307927063\n",
            "iteration 346 :train_loss:0.43891080921281145 val_loss0.461985001285138\n",
            "iteration 347 :train_loss:0.43876707912944884 val_loss0.46183115987366696\n",
            "iteration 348 :train_loss:0.43862453008571906 val_loss0.46167772311498617\n",
            "iteration 349 :train_loss:0.4384831673789909 val_loss0.4615251668658399\n",
            "iteration 350 :train_loss:0.4383426598772983 val_loss0.4613744598282884\n",
            "iteration 351 :train_loss:0.43820319565550914 val_loss0.46122489390736154\n",
            "iteration 352 :train_loss:0.4380650045474607 val_loss0.46107654695682093\n",
            "iteration 353 :train_loss:0.4379280479724083 val_loss0.46092877587247943\n",
            "iteration 354 :train_loss:0.4377924875769835 val_loss0.46078247578768994\n",
            "iteration 355 :train_loss:0.4376581507200397 val_loss0.46063486201855885\n",
            "iteration 356 :train_loss:0.4375250413560444 val_loss0.46048972407202127\n",
            "iteration 357 :train_loss:0.4373927498650479 val_loss0.4603440022217373\n",
            "iteration 358 :train_loss:0.43726141368925564 val_loss0.4602003354534984\n",
            "iteration 359 :train_loss:0.4371312236405926 val_loss0.4600579968193464\n",
            "iteration 360 :train_loss:0.4370019956906129 val_loss0.45991668728047014\n",
            "iteration 361 :train_loss:0.4368736881367357 val_loss0.4597762273456214\n",
            "iteration 362 :train_loss:0.43674615288041774 val_loss0.4596372638678923\n",
            "iteration 363 :train_loss:0.4366193738118814 val_loss0.4595004933060595\n",
            "iteration 364 :train_loss:0.436493852961453 val_loss0.4593640558336369\n",
            "iteration 365 :train_loss:0.4363695267335111 val_loss0.45922890296963054\n",
            "iteration 366 :train_loss:0.4362459823814391 val_loss0.45909502438172384\n",
            "iteration 367 :train_loss:0.43612320593932036 val_loss0.45896262815228467\n",
            "iteration 368 :train_loss:0.43600131485250915 val_loss0.45883220211894854\n",
            "iteration 369 :train_loss:0.4358799672761133 val_loss0.45870244309746244\n",
            "iteration 370 :train_loss:0.43575941267499996 val_loss0.45857336374424507\n",
            "iteration 371 :train_loss:0.4356396467234665 val_loss0.45844479411070893\n",
            "iteration 372 :train_loss:0.4355207099185429 val_loss0.4583176153831236\n",
            "iteration 373 :train_loss:0.4354023892744567 val_loss0.45819204845046785\n",
            "iteration 374 :train_loss:0.4352847823233044 val_loss0.4580673050092847\n",
            "iteration 375 :train_loss:0.43516792378589836 val_loss0.45794392931651773\n",
            "iteration 376 :train_loss:0.43505169042690867 val_loss0.4578222045319047\n",
            "iteration 377 :train_loss:0.4349363982435296 val_loss0.45770052690886137\n",
            "iteration 378 :train_loss:0.4348219957152876 val_loss0.457579864826699\n",
            "iteration 379 :train_loss:0.43470842339045807 val_loss0.45746013257688684\n",
            "iteration 380 :train_loss:0.43459535948413597 val_loss0.45734136299656686\n",
            "iteration 381 :train_loss:0.4344828027577662 val_loss0.45722444144853713\n",
            "iteration 382 :train_loss:0.43437109365718385 val_loss0.4571082686751615\n",
            "iteration 383 :train_loss:0.4342597289170715 val_loss0.45699381489006563\n",
            "iteration 384 :train_loss:0.4341491057162861 val_loss0.45687983701309165\n",
            "iteration 385 :train_loss:0.4340392317781406 val_loss0.4567660898347721\n",
            "iteration 386 :train_loss:0.43392976013954476 val_loss0.45665480611387815\n",
            "iteration 387 :train_loss:0.4338209691559061 val_loss0.45654258358093114\n",
            "iteration 388 :train_loss:0.43371313141721857 val_loss0.4564303552590348\n",
            "iteration 389 :train_loss:0.43360610099155766 val_loss0.45631765109372385\n",
            "iteration 390 :train_loss:0.4334995179044275 val_loss0.4562065758289017\n",
            "iteration 391 :train_loss:0.43339341760238215 val_loss0.45609701004215153\n",
            "iteration 392 :train_loss:0.4332875959772825 val_loss0.45598846809572435\n",
            "iteration 393 :train_loss:0.4331824726045406 val_loss0.4558809455084681\n",
            "iteration 394 :train_loss:0.4330780983862213 val_loss0.4557733581414626\n",
            "iteration 395 :train_loss:0.4329738641197592 val_loss0.4556681386387785\n",
            "iteration 396 :train_loss:0.4328699523672274 val_loss0.4555611822214568\n",
            "iteration 397 :train_loss:0.4327662809200429 val_loss0.4554555311656314\n",
            "iteration 398 :train_loss:0.4326630155323645 val_loss0.45535118543042236\n",
            "iteration 399 :train_loss:0.43256004708955975 val_loss0.4552485199142003\n",
            "iteration 400 :train_loss:0.432457785385142 val_loss0.4551462714412351\n",
            "iteration 401 :train_loss:0.43235623162230885 val_loss0.45504458645737883\n",
            "iteration 402 :train_loss:0.43225539733052576 val_loss0.45494317292803943\n",
            "iteration 403 :train_loss:0.4321551544879293 val_loss0.454841920661202\n",
            "iteration 404 :train_loss:0.4320553480524108 val_loss0.45474041041324853\n",
            "iteration 405 :train_loss:0.4319557696313029 val_loss0.45463982095889693\n",
            "iteration 406 :train_loss:0.4318567649273413 val_loss0.45454093019989983\n",
            "iteration 407 :train_loss:0.43175888600273443 val_loss0.4544404089390456\n",
            "iteration 408 :train_loss:0.43166151586898194 val_loss0.45434075212835906\n",
            "iteration 409 :train_loss:0.43156459901350086 val_loss0.45424144944232636\n",
            "iteration 410 :train_loss:0.43146796747935345 val_loss0.4541435830651915\n",
            "iteration 411 :train_loss:0.4313719731176479 val_loss0.4540463708320492\n",
            "iteration 412 :train_loss:0.43127652415513745 val_loss0.4539493207736201\n",
            "iteration 413 :train_loss:0.4311813428485422 val_loss0.4538536941129234\n",
            "iteration 414 :train_loss:0.4310869298119806 val_loss0.4537589291059171\n",
            "iteration 415 :train_loss:0.4309931594588753 val_loss0.45366409147522235\n",
            "iteration 416 :train_loss:0.4308996993477238 val_loss0.45356928291124615\n",
            "iteration 417 :train_loss:0.4308069417362698 val_loss0.45347434711165063\n",
            "iteration 418 :train_loss:0.43071482830691427 val_loss0.4533796860987551\n",
            "iteration 419 :train_loss:0.43062349665587946 val_loss0.45328508992913796\n",
            "iteration 420 :train_loss:0.43053275372776784 val_loss0.4531903632108405\n",
            "iteration 421 :train_loss:0.4304424160222859 val_loss0.4530961097512299\n",
            "iteration 422 :train_loss:0.4303525821964855 val_loss0.4530035548719945\n",
            "iteration 423 :train_loss:0.4302632940985775 val_loss0.45291099802100687\n",
            "iteration 424 :train_loss:0.4301743907131383 val_loss0.45281947929468624\n",
            "iteration 425 :train_loss:0.4300858422297084 val_loss0.4527268984079656\n",
            "iteration 426 :train_loss:0.42999737939670224 val_loss0.45263574636173565\n",
            "iteration 427 :train_loss:0.4299092448216995 val_loss0.4525446340715927\n",
            "iteration 428 :train_loss:0.42982163719927446 val_loss0.4524523121915603\n",
            "iteration 429 :train_loss:0.4297345110585125 val_loss0.45236197868628064\n",
            "iteration 430 :train_loss:0.4296477076436405 val_loss0.45227068155463884\n",
            "iteration 431 :train_loss:0.4295611915744691 val_loss0.4521793321385129\n",
            "iteration 432 :train_loss:0.4294748029610238 val_loss0.45208968240745134\n",
            "iteration 433 :train_loss:0.4293885510854066 val_loss0.45200055642709686\n",
            "iteration 434 :train_loss:0.42930299796151444 val_loss0.45191201291459876\n",
            "iteration 435 :train_loss:0.4292179004532673 val_loss0.4518225578490095\n",
            "iteration 436 :train_loss:0.42913326213836434 val_loss0.45173410870835373\n",
            "iteration 437 :train_loss:0.429048978586716 val_loss0.4516463688906982\n",
            "iteration 438 :train_loss:0.4289647751752391 val_loss0.4515585571689761\n",
            "iteration 439 :train_loss:0.42888106127522335 val_loss0.4514712537311058\n",
            "iteration 440 :train_loss:0.4287976806827763 val_loss0.45138483976979676\n",
            "iteration 441 :train_loss:0.42871415585997386 val_loss0.4512993028100115\n",
            "iteration 442 :train_loss:0.42863122036146256 val_loss0.45121203971758833\n",
            "iteration 443 :train_loss:0.42854870830587616 val_loss0.4511263331045002\n",
            "iteration 444 :train_loss:0.42846620068235136 val_loss0.4510406762665554\n",
            "iteration 445 :train_loss:0.4283842351631463 val_loss0.4509558550434484\n",
            "iteration 446 :train_loss:0.42830289481849054 val_loss0.45087060812485075\n",
            "iteration 447 :train_loss:0.4282220926999644 val_loss0.4507866100630421\n",
            "iteration 448 :train_loss:0.4281416681427433 val_loss0.4507025734040716\n",
            "iteration 449 :train_loss:0.42806176650484556 val_loss0.4506184524926372\n",
            "iteration 450 :train_loss:0.4279822599869407 val_loss0.45053368949851463\n",
            "iteration 451 :train_loss:0.4279033589875105 val_loss0.45045161190834493\n",
            "iteration 452 :train_loss:0.4278249179235463 val_loss0.4503682485446827\n",
            "iteration 453 :train_loss:0.4277467695079909 val_loss0.45028488225625\n",
            "iteration 454 :train_loss:0.4276689208284193 val_loss0.4502030391985492\n",
            "iteration 455 :train_loss:0.42759175291146795 val_loss0.4501218776370517\n",
            "iteration 456 :train_loss:0.42751475410776557 val_loss0.45003558472123273\n",
            "iteration 457 :train_loss:0.42743808603367234 val_loss0.44995601267777974\n",
            "iteration 458 :train_loss:0.42736128146026736 val_loss0.4498702941461076\n",
            "iteration 459 :train_loss:0.42728447884389614 val_loss0.44978983237969616\n",
            "iteration 460 :train_loss:0.4272080014997988 val_loss0.4497118108066476\n",
            "iteration 461 :train_loss:0.4271315332389008 val_loss0.44962852787976904\n",
            "iteration 462 :train_loss:0.4270552657371707 val_loss0.44955155456826745\n",
            "iteration 463 :train_loss:0.42697897013941305 val_loss0.44946843260094643\n",
            "iteration 464 :train_loss:0.42690307011523665 val_loss0.44939280185021263\n",
            "iteration 465 :train_loss:0.42682754894241387 val_loss0.44931058243189476\n",
            "iteration 466 :train_loss:0.42675252121696455 val_loss0.4492342523763198\n",
            "iteration 467 :train_loss:0.4266775635289204 val_loss0.44915212512280017\n",
            "iteration 468 :train_loss:0.4266029207766753 val_loss0.44907736600580056\n",
            "iteration 469 :train_loss:0.4265284227409222 val_loss0.44899613239167196\n",
            "iteration 470 :train_loss:0.42645403128465464 val_loss0.44892120370426575\n",
            "iteration 471 :train_loss:0.42637983172786115 val_loss0.4488401811119807\n",
            "iteration 472 :train_loss:0.4263062656048821 val_loss0.44876161356910493\n",
            "iteration 473 :train_loss:0.4262331708672282 val_loss0.448685953276308\n",
            "iteration 474 :train_loss:0.42616033397010233 val_loss0.44860564401886976\n",
            "iteration 475 :train_loss:0.42608793507260956 val_loss0.44852952002915863\n",
            "iteration 476 :train_loss:0.42601594148350247 val_loss0.44845553103172275\n",
            "iteration 477 :train_loss:0.4259442068803396 val_loss0.4483762704700876\n",
            "iteration 478 :train_loss:0.4258726145520219 val_loss0.4483010448585787\n",
            "iteration 479 :train_loss:0.42580108901689245 val_loss0.4482249144663169\n",
            "iteration 480 :train_loss:0.42572970791839376 val_loss0.4481490445378451\n",
            "iteration 481 :train_loss:0.42565861663386073 val_loss0.44807293328862385\n",
            "iteration 482 :train_loss:0.4255878751293497 val_loss0.44800019093862464\n",
            "iteration 483 :train_loss:0.4255173888730128 val_loss0.44792282416744705\n",
            "iteration 484 :train_loss:0.4254473511053081 val_loss0.4478467309857288\n",
            "iteration 485 :train_loss:0.42537746217148736 val_loss0.4477721331913899\n",
            "iteration 486 :train_loss:0.4253078941276769 val_loss0.44769894409154054\n",
            "iteration 487 :train_loss:0.4252386333298447 val_loss0.44762607352366535\n",
            "iteration 488 :train_loss:0.42516952659560875 val_loss0.4475531063136704\n",
            "iteration 489 :train_loss:0.42510063560629535 val_loss0.44747971983048607\n",
            "iteration 490 :train_loss:0.42503202949806584 val_loss0.4474064857328341\n",
            "iteration 491 :train_loss:0.4249636932962949 val_loss0.447334357993065\n",
            "iteration 492 :train_loss:0.4248951705002399 val_loss0.4472615768396804\n",
            "iteration 493 :train_loss:0.4248266118466787 val_loss0.44718997029442537\n",
            "iteration 494 :train_loss:0.42475838985041603 val_loss0.4471175571071363\n",
            "iteration 495 :train_loss:0.424690313560893 val_loss0.4470451166550567\n",
            "iteration 496 :train_loss:0.42462243894240964 val_loss0.44697395624586284\n",
            "iteration 497 :train_loss:0.4245543984253623 val_loss0.44690188168671274\n",
            "iteration 498 :train_loss:0.424486700066616 val_loss0.44682946171247384\n",
            "iteration 499 :train_loss:0.4244190528407184 val_loss0.44675894477749717\n",
            "iteration 500 :train_loss:0.4243517671524706 val_loss0.4466850762148576\n",
            "iteration 501 :train_loss:0.42428470668567203 val_loss0.4466133511980903\n",
            "iteration 502 :train_loss:0.42421776644428755 val_loss0.4465417452102921\n",
            "iteration 503 :train_loss:0.4241512255316852 val_loss0.44647139224040583\n",
            "iteration 504 :train_loss:0.42408465964496794 val_loss0.4463986170298176\n",
            "iteration 505 :train_loss:0.42401825531533677 val_loss0.44632827293550176\n",
            "iteration 506 :train_loss:0.423951885522734 val_loss0.4462570074901747\n",
            "iteration 507 :train_loss:0.4238852477806881 val_loss0.44618746985740615\n",
            "iteration 508 :train_loss:0.42381890483455353 val_loss0.4461160233983871\n",
            "iteration 509 :train_loss:0.42375281861987557 val_loss0.4460477605639481\n",
            "iteration 510 :train_loss:0.423686847706318 val_loss0.4459779908134178\n",
            "iteration 511 :train_loss:0.423620781172252 val_loss0.44590917137712055\n",
            "iteration 512 :train_loss:0.423554912068178 val_loss0.44583997095854655\n",
            "iteration 513 :train_loss:0.42348929666578305 val_loss0.44577282770301296\n",
            "iteration 514 :train_loss:0.4234237149092524 val_loss0.44570281098447884\n",
            "iteration 515 :train_loss:0.42335830247997214 val_loss0.44563509409041413\n",
            "iteration 516 :train_loss:0.42329314204432605 val_loss0.44556583015263485\n",
            "iteration 517 :train_loss:0.4232279861156847 val_loss0.4454975109601934\n",
            "iteration 518 :train_loss:0.4231629342129955 val_loss0.4454286177457013\n",
            "iteration 519 :train_loss:0.4230980099458178 val_loss0.44536155868398775\n",
            "iteration 520 :train_loss:0.42303313181675156 val_loss0.4452934427588387\n",
            "iteration 521 :train_loss:0.4229682724519495 val_loss0.4452245160656802\n",
            "iteration 522 :train_loss:0.42290348330943384 val_loss0.4451580469822775\n",
            "iteration 523 :train_loss:0.4228387931950054 val_loss0.44508856323988805\n",
            "iteration 524 :train_loss:0.42277413692044796 val_loss0.44502094437098666\n",
            "iteration 525 :train_loss:0.4227095164785695 val_loss0.44495113145861803\n",
            "iteration 526 :train_loss:0.422645061332073 val_loss0.4448812304394735\n",
            "iteration 527 :train_loss:0.42258085452726335 val_loss0.44481327057499087\n",
            "iteration 528 :train_loss:0.4225169571157753 val_loss0.44474464829288146\n",
            "iteration 529 :train_loss:0.42245332786328077 val_loss0.44467662119352497\n",
            "iteration 530 :train_loss:0.4223899948034989 val_loss0.4446100273094173\n",
            "iteration 531 :train_loss:0.4223267619282795 val_loss0.44454116534113536\n",
            "iteration 532 :train_loss:0.4222637130702275 val_loss0.4444735678895133\n",
            "iteration 533 :train_loss:0.422200742993839 val_loss0.44440574907497804\n",
            "iteration 534 :train_loss:0.4221379592339899 val_loss0.4443399310274161\n",
            "iteration 535 :train_loss:0.42207541787222314 val_loss0.44427082526156025\n",
            "iteration 536 :train_loss:0.422013024974813 val_loss0.44420255349750376\n",
            "iteration 537 :train_loss:0.42195075532724313 val_loss0.4441358002563999\n",
            "iteration 538 :train_loss:0.42188880579470533 val_loss0.44406925514199264\n",
            "iteration 539 :train_loss:0.42182708677534475 val_loss0.44400169168870857\n",
            "iteration 540 :train_loss:0.4217652731670214 val_loss0.4439355926106319\n",
            "iteration 541 :train_loss:0.4217036037735964 val_loss0.44386869766210374\n",
            "iteration 542 :train_loss:0.4216419244802853 val_loss0.44380319165163934\n",
            "iteration 543 :train_loss:0.4215803820462524 val_loss0.4437377651544697\n",
            "iteration 544 :train_loss:0.4215187074493066 val_loss0.4436721351895759\n",
            "iteration 545 :train_loss:0.42145703446324195 val_loss0.44360868960608524\n",
            "iteration 546 :train_loss:0.42139532832201093 val_loss0.4435420181377539\n",
            "iteration 547 :train_loss:0.4213336390466268 val_loss0.44347642707113194\n",
            "iteration 548 :train_loss:0.4212718736673471 val_loss0.44341224541460206\n",
            "iteration 549 :train_loss:0.4212101639848955 val_loss0.4433457834045807\n",
            "iteration 550 :train_loss:0.4211484418762889 val_loss0.4432823093168994\n",
            "iteration 551 :train_loss:0.4210864879279718 val_loss0.4432159005845103\n",
            "iteration 552 :train_loss:0.4210243981609012 val_loss0.4431514547234618\n",
            "iteration 553 :train_loss:0.42096223012757517 val_loss0.4430877418386769\n",
            "iteration 554 :train_loss:0.42090013507445406 val_loss0.4430219544037643\n",
            "iteration 555 :train_loss:0.42083835012582477 val_loss0.4429570360382378\n",
            "iteration 556 :train_loss:0.42077662683442413 val_loss0.44289160767686164\n",
            "iteration 557 :train_loss:0.4207149999358389 val_loss0.4428276531907001\n",
            "iteration 558 :train_loss:0.4206534964246547 val_loss0.44276305086748213\n",
            "iteration 559 :train_loss:0.42059207986602565 val_loss0.4426982673360172\n",
            "iteration 560 :train_loss:0.42053081791778296 val_loss0.44263276056677126\n",
            "iteration 561 :train_loss:0.4204696862857399 val_loss0.4425684092371448\n",
            "iteration 562 :train_loss:0.4204087736165164 val_loss0.44250391899483726\n",
            "iteration 563 :train_loss:0.42034789868010886 val_loss0.44243928992191606\n",
            "iteration 564 :train_loss:0.4202871238256756 val_loss0.4423751440633686\n",
            "iteration 565 :train_loss:0.420226380167258 val_loss0.44231021655062197\n",
            "iteration 566 :train_loss:0.42016571435506045 val_loss0.44224696872497654\n",
            "iteration 567 :train_loss:0.4201052938678815 val_loss0.44218296785255573\n",
            "iteration 568 :train_loss:0.42004498555084835 val_loss0.4421182621420002\n",
            "iteration 569 :train_loss:0.41998446972929215 val_loss0.44205578857081396\n",
            "iteration 570 :train_loss:0.4199241283114449 val_loss0.4419917736745296\n",
            "iteration 571 :train_loss:0.41986391479047225 val_loss0.44192856107422446\n",
            "iteration 572 :train_loss:0.41980391508744364 val_loss0.4418644315520917\n",
            "iteration 573 :train_loss:0.4197438256498561 val_loss0.44180297478346037\n",
            "iteration 574 :train_loss:0.41968360405828753 val_loss0.44174048504732505\n",
            "iteration 575 :train_loss:0.41962329677332166 val_loss0.44167583231193125\n",
            "iteration 576 :train_loss:0.419562861158097 val_loss0.4416138496226047\n",
            "iteration 577 :train_loss:0.41950246197313107 val_loss0.4415497682313975\n",
            "iteration 578 :train_loss:0.41944230490503925 val_loss0.44148731668333363\n",
            "iteration 579 :train_loss:0.4193820667316921 val_loss0.4414236201276424\n",
            "iteration 580 :train_loss:0.41932193542373686 val_loss0.4413576418876058\n",
            "iteration 581 :train_loss:0.4192620237665179 val_loss0.4412947636854099\n",
            "iteration 582 :train_loss:0.4192020473590125 val_loss0.44122945307714134\n",
            "iteration 583 :train_loss:0.4191422511754257 val_loss0.4411655605146909\n",
            "iteration 584 :train_loss:0.419082519774416 val_loss0.44109960340483695\n",
            "iteration 585 :train_loss:0.41902304037776034 val_loss0.4410360674288541\n",
            "iteration 586 :train_loss:0.41896367816495805 val_loss0.44097090841574027\n",
            "iteration 587 :train_loss:0.41890435946775284 val_loss0.44090755300706497\n",
            "iteration 588 :train_loss:0.418844926633146 val_loss0.44084505766607324\n",
            "iteration 589 :train_loss:0.41878564546012725 val_loss0.4407803481622133\n",
            "iteration 590 :train_loss:0.4187266331247026 val_loss0.44071554563672843\n",
            "iteration 591 :train_loss:0.41866779468734383 val_loss0.44065116803228355\n",
            "iteration 592 :train_loss:0.4186088772864897 val_loss0.4405878422713098\n",
            "iteration 593 :train_loss:0.41854987928623183 val_loss0.4405259853375087\n",
            "iteration 594 :train_loss:0.4184907816962509 val_loss0.44046160857446454\n",
            "iteration 595 :train_loss:0.41843191235588717 val_loss0.4403987597229723\n",
            "iteration 596 :train_loss:0.4183728894194893 val_loss0.440334583843107\n",
            "iteration 597 :train_loss:0.41831392571377507 val_loss0.4402719405066456\n",
            "iteration 598 :train_loss:0.4182547646666779 val_loss0.4402077234097654\n",
            "iteration 599 :train_loss:0.41819551746843375 val_loss0.44014406102371806\n",
            "iteration 600 :train_loss:0.4181361428603877 val_loss0.44008187022277206\n",
            "iteration 601 :train_loss:0.4180768365197975 val_loss0.4400175833165024\n",
            "iteration 602 :train_loss:0.41801777412431057 val_loss0.4399551656774723\n",
            "iteration 603 :train_loss:0.4179585984161893 val_loss0.4398936925533534\n",
            "iteration 604 :train_loss:0.4178997037375881 val_loss0.4398316940939068\n",
            "iteration 605 :train_loss:0.4178408114312798 val_loss0.4397702946893966\n",
            "iteration 606 :train_loss:0.41778187056177846 val_loss0.4397088061621299\n",
            "iteration 607 :train_loss:0.4177229963732929 val_loss0.4396498935243173\n",
            "iteration 608 :train_loss:0.41766415613821656 val_loss0.4395873732251377\n",
            "iteration 609 :train_loss:0.4176052934166998 val_loss0.439526550617764\n",
            "iteration 610 :train_loss:0.4175464276453163 val_loss0.4394648347088744\n",
            "iteration 611 :train_loss:0.4174875188116865 val_loss0.43940510959496787\n",
            "iteration 612 :train_loss:0.41742856075599155 val_loss0.439341362301012\n",
            "iteration 613 :train_loss:0.4173696949676761 val_loss0.439280197564213\n",
            "iteration 614 :train_loss:0.41731073697594046 val_loss0.43921890055108415\n",
            "iteration 615 :train_loss:0.4172520532395509 val_loss0.43915984655131746\n",
            "iteration 616 :train_loss:0.4171935518180101 val_loss0.43909493701317903\n",
            "iteration 617 :train_loss:0.4171353018114233 val_loss0.43903249201420863\n",
            "iteration 618 :train_loss:0.4170771283357883 val_loss0.4389693307004877\n",
            "iteration 619 :train_loss:0.4170188099091679 val_loss0.43890674552909864\n",
            "iteration 620 :train_loss:0.4169603385802385 val_loss0.43884387261821844\n",
            "iteration 621 :train_loss:0.4169020999885736 val_loss0.4387830420222381\n",
            "iteration 622 :train_loss:0.4168440844168682 val_loss0.43872202501747976\n",
            "iteration 623 :train_loss:0.416786229801113 val_loss0.438659072343209\n",
            "iteration 624 :train_loss:0.416728615528337 val_loss0.4385978774710406\n",
            "iteration 625 :train_loss:0.41667128308005946 val_loss0.4385371830912925\n",
            "iteration 626 :train_loss:0.41661410912685176 val_loss0.43847209166641354\n",
            "iteration 627 :train_loss:0.41655709890390236 val_loss0.4384140887704635\n",
            "iteration 628 :train_loss:0.4165001128279281 val_loss0.4383477482189342\n",
            "iteration 629 :train_loss:0.4164433436969466 val_loss0.43828372235073704\n",
            "iteration 630 :train_loss:0.41638690284226665 val_loss0.43822678427286915\n",
            "iteration 631 :train_loss:0.41633039362164986 val_loss0.43816102345492414\n",
            "iteration 632 :train_loss:0.41627401433529254 val_loss0.4380970692251496\n",
            "iteration 633 :train_loss:0.41621786116676096 val_loss0.4380357160855138\n",
            "iteration 634 :train_loss:0.4161618477827096 val_loss0.43797291358394286\n",
            "iteration 635 :train_loss:0.41610597348408945 val_loss0.4379109253035293\n",
            "iteration 636 :train_loss:0.416050307505933 val_loss0.43784942382259334\n",
            "iteration 637 :train_loss:0.4159948958982647 val_loss0.4377871939258519\n",
            "iteration 638 :train_loss:0.41593968070963483 val_loss0.4377238152388379\n",
            "iteration 639 :train_loss:0.4158846207609238 val_loss0.4376660051196884\n",
            "iteration 640 :train_loss:0.41582936878580296 val_loss0.4376013388748995\n",
            "iteration 641 :train_loss:0.4157743388629124 val_loss0.437538001202415\n",
            "iteration 642 :train_loss:0.41571935669268506 val_loss0.43748058396272466\n",
            "iteration 643 :train_loss:0.415664239127994 val_loss0.4374154129430928\n",
            "iteration 644 :train_loss:0.4156094911486385 val_loss0.4373518136651545\n",
            "iteration 645 :train_loss:0.4155551065642744 val_loss0.43729459871060894\n",
            "iteration 646 :train_loss:0.41550076696781946 val_loss0.43723069360270617\n",
            "iteration 647 :train_loss:0.41544653574872226 val_loss0.43716900358709165\n",
            "iteration 648 :train_loss:0.41539245041292555 val_loss0.43711213174818175\n",
            "iteration 649 :train_loss:0.41533854591177727 val_loss0.43704849980579796\n",
            "iteration 650 :train_loss:0.4152846602894933 val_loss0.43698664689866823\n",
            "iteration 651 :train_loss:0.41523108404000014 val_loss0.436926289213311\n",
            "iteration 652 :train_loss:0.4151773727101972 val_loss0.436867002666878\n",
            "iteration 653 :train_loss:0.4151237357748933 val_loss0.43681209964119055\n",
            "iteration 654 :train_loss:0.41506969884133055 val_loss0.43674873506652884\n",
            "iteration 655 :train_loss:0.4150160809289376 val_loss0.4366880016741795\n",
            "iteration 656 :train_loss:0.4149626521870064 val_loss0.4366265118835481\n",
            "iteration 657 :train_loss:0.4149095228878312 val_loss0.4365692947749161\n",
            "iteration 658 :train_loss:0.41485654863716986 val_loss0.436505223458568\n",
            "iteration 659 :train_loss:0.41480380350672474 val_loss0.4364451149517267\n",
            "iteration 660 :train_loss:0.4147511252399223 val_loss0.43638651743595064\n",
            "iteration 661 :train_loss:0.4146985728325916 val_loss0.4363304468777644\n",
            "iteration 662 :train_loss:0.41464603008509393 val_loss0.43626742311221767\n",
            "iteration 663 :train_loss:0.4145939542266674 val_loss0.4362086307524122\n",
            "iteration 664 :train_loss:0.4145421139180952 val_loss0.43614932464656164\n",
            "iteration 665 :train_loss:0.41449016272407574 val_loss0.43609288845877725\n",
            "iteration 666 :train_loss:0.4144383432087663 val_loss0.43603032496425903\n",
            "iteration 667 :train_loss:0.41438674196189546 val_loss0.43597034107904215\n",
            "iteration 668 :train_loss:0.41433533756410507 val_loss0.4359097824347355\n",
            "iteration 669 :train_loss:0.41428424944967057 val_loss0.43585041153071724\n",
            "iteration 670 :train_loss:0.4142334340127038 val_loss0.43578797913173617\n",
            "iteration 671 :train_loss:0.4141829600626707 val_loss0.4357287713271544\n",
            "iteration 672 :train_loss:0.41413273617602625 val_loss0.4356686299792869\n",
            "iteration 673 :train_loss:0.414082729990543 val_loss0.4356068225174961\n",
            "iteration 674 :train_loss:0.414032682197263 val_loss0.4355476649278008\n",
            "iteration 675 :train_loss:0.41398230684182985 val_loss0.4354906825982465\n",
            "iteration 676 :train_loss:0.41393192818355945 val_loss0.4354284991735271\n",
            "iteration 677 :train_loss:0.41388175989217196 val_loss0.43536597421605694\n",
            "iteration 678 :train_loss:0.41383188513248503 val_loss0.4353115478117232\n",
            "iteration 679 :train_loss:0.4137819118879394 val_loss0.4352507992019292\n",
            "iteration 680 :train_loss:0.41373212941242565 val_loss0.4351870492766024\n",
            "iteration 681 :train_loss:0.4136824146965586 val_loss0.43513068275314637\n",
            "iteration 682 :train_loss:0.413632811140546 val_loss0.435070171779158\n",
            "iteration 683 :train_loss:0.41358308276417666 val_loss0.43501426139208804\n",
            "iteration 684 :train_loss:0.4135335062795221 val_loss0.4349545267970314\n",
            "iteration 685 :train_loss:0.41348387452514673 val_loss0.4348977176228219\n",
            "iteration 686 :train_loss:0.4134344994480505 val_loss0.4348388242849954\n",
            "iteration 687 :train_loss:0.41338541152424607 val_loss0.4347819976526982\n",
            "iteration 688 :train_loss:0.4133364847579745 val_loss0.4347256024344487\n",
            "iteration 689 :train_loss:0.4132876766966688 val_loss0.4346669549935222\n",
            "iteration 690 :train_loss:0.41323889617942716 val_loss0.4346104992171941\n",
            "iteration 691 :train_loss:0.4131904131940869 val_loss0.4345505718753742\n",
            "iteration 692 :train_loss:0.41314199686845204 val_loss0.4344975018822215\n",
            "iteration 693 :train_loss:0.41309369171269694 val_loss0.4344384285595829\n",
            "iteration 694 :train_loss:0.41304571014841146 val_loss0.4343805929904024\n",
            "iteration 695 :train_loss:0.41299766854598674 val_loss0.43432786366663795\n",
            "iteration 696 :train_loss:0.41294975630569175 val_loss0.43426902153393987\n",
            "iteration 697 :train_loss:0.4129023741585716 val_loss0.43420865455696434\n",
            "iteration 698 :train_loss:0.4128549198952544 val_loss0.43415649691148855\n",
            "iteration 699 :train_loss:0.41280759262314015 val_loss0.43409687858573753\n",
            "iteration 700 :train_loss:0.4127605773652781 val_loss0.4340414985227098\n",
            "iteration 701 :train_loss:0.41271342761636115 val_loss0.4339828839508436\n",
            "iteration 702 :train_loss:0.412666349814238 val_loss0.43392511355120983\n",
            "iteration 703 :train_loss:0.4126192274736783 val_loss0.4338699353158126\n",
            "iteration 704 :train_loss:0.41257195243971634 val_loss0.4338172344189289\n",
            "iteration 705 :train_loss:0.41252486596336346 val_loss0.43375818008954226\n",
            "iteration 706 :train_loss:0.41247784537106147 val_loss0.4337024742873229\n",
            "iteration 707 :train_loss:0.41243099144903034 val_loss0.4336491774871659\n",
            "iteration 708 :train_loss:0.4123841133876906 val_loss0.4335903977892417\n",
            "iteration 709 :train_loss:0.41233732726465294 val_loss0.43353417272260064\n",
            "iteration 710 :train_loss:0.4122905701032381 val_loss0.433479991737105\n",
            "iteration 711 :train_loss:0.41224404947721166 val_loss0.4334223896430735\n",
            "iteration 712 :train_loss:0.41219774504875323 val_loss0.4333682273769215\n",
            "iteration 713 :train_loss:0.41215158889012304 val_loss0.43331191549327136\n",
            "iteration 714 :train_loss:0.4121056673299392 val_loss0.4332571014097141\n",
            "iteration 715 :train_loss:0.4120596722201534 val_loss0.433201741779918\n",
            "iteration 716 :train_loss:0.4120137905107203 val_loss0.4331461506663171\n",
            "iteration 717 :train_loss:0.4119680510205838 val_loss0.4330968629751978\n",
            "iteration 718 :train_loss:0.4119220554526325 val_loss0.4330380793852593\n",
            "iteration 719 :train_loss:0.41187632226892157 val_loss0.4329805053167999\n",
            "iteration 720 :train_loss:0.4118305931042387 val_loss0.4329256848580101\n",
            "iteration 721 :train_loss:0.4117849221952545 val_loss0.43287077645510363\n",
            "iteration 722 :train_loss:0.41173933870427115 val_loss0.43281706798206654\n",
            "iteration 723 :train_loss:0.41169376936057644 val_loss0.4327647070040956\n",
            "iteration 724 :train_loss:0.41164831896665754 val_loss0.43270950418852727\n",
            "iteration 725 :train_loss:0.4116031768305205 val_loss0.4326537614809976\n",
            "iteration 726 :train_loss:0.41155805940369233 val_loss0.4326001860572396\n",
            "iteration 727 :train_loss:0.4115131406845525 val_loss0.43254375274148527\n",
            "iteration 728 :train_loss:0.41146835129013715 val_loss0.43248698154175313\n",
            "iteration 729 :train_loss:0.4114237403911284 val_loss0.4324347447987655\n",
            "iteration 730 :train_loss:0.4113791526851613 val_loss0.4323779921681517\n",
            "iteration 731 :train_loss:0.4113345342573199 val_loss0.4323258469085325\n",
            "iteration 732 :train_loss:0.4112898787016745 val_loss0.4322686235131957\n",
            "iteration 733 :train_loss:0.4112454649994127 val_loss0.432212297915032\n",
            "iteration 734 :train_loss:0.4112012272943062 val_loss0.432160872934725\n",
            "iteration 735 :train_loss:0.411156996284067 val_loss0.4321037944491828\n",
            "iteration 736 :train_loss:0.41111289657022654 val_loss0.43204898295764266\n",
            "iteration 737 :train_loss:0.4110688538572272 val_loss0.4319941318698226\n",
            "iteration 738 :train_loss:0.4110247919766826 val_loss0.4319428025221176\n",
            "iteration 739 :train_loss:0.4109807468006994 val_loss0.4318890289700373\n",
            "iteration 740 :train_loss:0.4109367856179772 val_loss0.4318341163023512\n",
            "iteration 741 :train_loss:0.4108927950667633 val_loss0.43178611051914495\n",
            "iteration 742 :train_loss:0.41084863283151557 val_loss0.4317318150174362\n",
            "iteration 743 :train_loss:0.41080461114189104 val_loss0.43167854594304017\n",
            "iteration 744 :train_loss:0.41076072653688667 val_loss0.4316254319075566\n",
            "iteration 745 :train_loss:0.41071705250218316 val_loss0.43157555435644807\n",
            "iteration 746 :train_loss:0.41067353434499326 val_loss0.431519797074111\n",
            "iteration 747 :train_loss:0.410630095683984 val_loss0.43146481471208603\n",
            "iteration 748 :train_loss:0.4105866931677938 val_loss0.4314111346485609\n",
            "iteration 749 :train_loss:0.4105433897158422 val_loss0.43136027642631825\n",
            "iteration 750 :train_loss:0.41050043904950084 val_loss0.43130561168127396\n",
            "iteration 751 :train_loss:0.4104577488696943 val_loss0.43125209391860037\n",
            "iteration 752 :train_loss:0.410415039636671 val_loss0.4311997924424864\n",
            "iteration 753 :train_loss:0.4103723898418403 val_loss0.4311468376363141\n",
            "iteration 754 :train_loss:0.4103298350792901 val_loss0.4310949655876371\n",
            "iteration 755 :train_loss:0.410287300415788 val_loss0.43104531992042033\n",
            "iteration 756 :train_loss:0.4102445793037181 val_loss0.4309916702492487\n",
            "iteration 757 :train_loss:0.410201854932551 val_loss0.43093835647666073\n",
            "iteration 758 :train_loss:0.41015901022199047 val_loss0.43088512158845726\n",
            "iteration 759 :train_loss:0.41011604914920696 val_loss0.4308366540328464\n",
            "iteration 760 :train_loss:0.4100730693787089 val_loss0.43078315014542456\n",
            "iteration 761 :train_loss:0.4100300017833945 val_loss0.4307313173974197\n",
            "iteration 762 :train_loss:0.409986985699523 val_loss0.43067885101810954\n",
            "iteration 763 :train_loss:0.4099439538349413 val_loss0.4306311791543073\n",
            "iteration 764 :train_loss:0.40990070975160314 val_loss0.43057810476795083\n",
            "iteration 765 :train_loss:0.40985738950842376 val_loss0.43052731770426705\n",
            "iteration 766 :train_loss:0.40981403486153684 val_loss0.43047534806445353\n",
            "iteration 767 :train_loss:0.4097707487305188 val_loss0.4304295559496166\n",
            "iteration 768 :train_loss:0.40972730396869855 val_loss0.43037606578734494\n",
            "iteration 769 :train_loss:0.4096839556896344 val_loss0.4303216747748836\n",
            "iteration 770 :train_loss:0.40964051966182763 val_loss0.43026898636912225\n",
            "iteration 771 :train_loss:0.4095971055003601 val_loss0.43021878339156977\n",
            "iteration 772 :train_loss:0.409553869744177 val_loss0.4301676371242943\n",
            "iteration 773 :train_loss:0.4095109099703979 val_loss0.4301163592149488\n",
            "iteration 774 :train_loss:0.40946816506323863 val_loss0.4300648812829941\n",
            "iteration 775 :train_loss:0.4094254850515458 val_loss0.4300140602004561\n",
            "iteration 776 :train_loss:0.40938281306013347 val_loss0.4299642679854006\n",
            "iteration 777 :train_loss:0.40934023634892447 val_loss0.42991403666979855\n",
            "iteration 778 :train_loss:0.40929771355527034 val_loss0.42986292052851627\n",
            "iteration 779 :train_loss:0.40925509270387533 val_loss0.429813351361599\n",
            "iteration 780 :train_loss:0.40921239307139917 val_loss0.4297622724429703\n",
            "iteration 781 :train_loss:0.4091697935951773 val_loss0.4297121402908986\n",
            "iteration 782 :train_loss:0.4091273519849848 val_loss0.42966072705906644\n",
            "iteration 783 :train_loss:0.4090846963695912 val_loss0.42960955539803\n",
            "iteration 784 :train_loss:0.4090420777783729 val_loss0.4295573289918882\n",
            "iteration 785 :train_loss:0.40899934814020916 val_loss0.42950488206470316\n",
            "iteration 786 :train_loss:0.4089566288401904 val_loss0.4294517650399321\n",
            "iteration 787 :train_loss:0.40891388411943425 val_loss0.42940122326493213\n",
            "iteration 788 :train_loss:0.4088712233275251 val_loss0.42934921084544864\n",
            "iteration 789 :train_loss:0.40882878512138154 val_loss0.429297888509362\n",
            "iteration 790 :train_loss:0.4087864517159633 val_loss0.4292482734876175\n",
            "iteration 791 :train_loss:0.4087442433124005 val_loss0.4291964243594831\n",
            "iteration 792 :train_loss:0.4087025157594688 val_loss0.4291475620051386\n",
            "iteration 793 :train_loss:0.40866100248971343 val_loss0.42909839461439525\n",
            "iteration 794 :train_loss:0.40861983583198597 val_loss0.42904806308102267\n",
            "iteration 795 :train_loss:0.40857882778476895 val_loss0.4289993815342916\n",
            "iteration 796 :train_loss:0.40853788208421665 val_loss0.4289497222710866\n",
            "iteration 797 :train_loss:0.40849703361367024 val_loss0.42890088323991166\n",
            "iteration 798 :train_loss:0.4084564150829121 val_loss0.42885068330327447\n",
            "iteration 799 :train_loss:0.4084161431345026 val_loss0.42880163993749204\n",
            "iteration 800 :train_loss:0.40837610700210464 val_loss0.4287514458193664\n",
            "iteration 801 :train_loss:0.4083360333073089 val_loss0.4287020170879703\n",
            "iteration 802 :train_loss:0.40829609577490195 val_loss0.428653500462981\n",
            "iteration 803 :train_loss:0.40825650255379964 val_loss0.4286046838672588\n",
            "iteration 804 :train_loss:0.4082169723340037 val_loss0.42855586433890897\n",
            "iteration 805 :train_loss:0.4081775407006328 val_loss0.4285065116531716\n",
            "iteration 806 :train_loss:0.40813812630553775 val_loss0.42845993826457124\n",
            "iteration 807 :train_loss:0.4080988073646574 val_loss0.4284108678815536\n",
            "iteration 808 :train_loss:0.4080595293027469 val_loss0.42836310400200195\n",
            "iteration 809 :train_loss:0.4080201027790275 val_loss0.42831393660027095\n",
            "iteration 810 :train_loss:0.4079808634742729 val_loss0.42826557565954004\n",
            "iteration 811 :train_loss:0.4079415994314244 val_loss0.4282169152525181\n",
            "iteration 812 :train_loss:0.40790239115351196 val_loss0.4281692321057163\n",
            "iteration 813 :train_loss:0.4078632517794367 val_loss0.42812058828835076\n",
            "iteration 814 :train_loss:0.40782418740970566 val_loss0.42806958692906455\n",
            "iteration 815 :train_loss:0.40778524083411677 val_loss0.4280213897877683\n",
            "iteration 816 :train_loss:0.407746458797317 val_loss0.42797136401073527\n",
            "iteration 817 :train_loss:0.40770785507043106 val_loss0.42792274587130824\n",
            "iteration 818 :train_loss:0.40766934583177566 val_loss0.42787432367505673\n",
            "iteration 819 :train_loss:0.4076308494581389 val_loss0.42782387155188484\n",
            "iteration 820 :train_loss:0.40759242509356985 val_loss0.42777649853968597\n",
            "iteration 821 :train_loss:0.40755416787770354 val_loss0.42772870908255295\n",
            "iteration 822 :train_loss:0.40751604188477075 val_loss0.4276795206196981\n",
            "iteration 823 :train_loss:0.40747775261484714 val_loss0.42763212538314666\n",
            "iteration 824 :train_loss:0.4074393841600915 val_loss0.42758467114288706\n",
            "iteration 825 :train_loss:0.4074009863470183 val_loss0.42753599367014405\n",
            "iteration 826 :train_loss:0.40736280258135094 val_loss0.42748942671981144\n",
            "iteration 827 :train_loss:0.4073246972599592 val_loss0.4274427176001264\n",
            "iteration 828 :train_loss:0.4072865574226023 val_loss0.42739393156247907\n",
            "iteration 829 :train_loss:0.4072483351347207 val_loss0.4273472762054638\n",
            "iteration 830 :train_loss:0.4072100536222421 val_loss0.42729917230254033\n",
            "iteration 831 :train_loss:0.40717179503324813 val_loss0.42724958669772856\n",
            "iteration 832 :train_loss:0.40713355736436485 val_loss0.42720238497073726\n",
            "iteration 833 :train_loss:0.40709525261730334 val_loss0.4271547654735803\n",
            "iteration 834 :train_loss:0.4070569056517473 val_loss0.4271079117545797\n",
            "iteration 835 :train_loss:0.4070183268092523 val_loss0.427060885431937\n",
            "iteration 836 :train_loss:0.40697982445068254 val_loss0.4270122792863505\n",
            "iteration 837 :train_loss:0.4069415214083991 val_loss0.42696633314456445\n",
            "iteration 838 :train_loss:0.40690323733179795 val_loss0.4269198869836564\n",
            "iteration 839 :train_loss:0.40686498951113365 val_loss0.4268716945579153\n",
            "iteration 840 :train_loss:0.4068267803730612 val_loss0.42682669418981944\n",
            "iteration 841 :train_loss:0.4067885416622591 val_loss0.42678084294704993\n",
            "iteration 842 :train_loss:0.406750249284076 val_loss0.42673426045753593\n",
            "iteration 843 :train_loss:0.40671204956146856 val_loss0.42668635784537\n",
            "iteration 844 :train_loss:0.40667389770437823 val_loss0.4266393943262381\n",
            "iteration 845 :train_loss:0.4066357408193942 val_loss0.42659111696497143\n",
            "iteration 846 :train_loss:0.4065976542569074 val_loss0.4265439680130151\n",
            "iteration 847 :train_loss:0.40655950659365114 val_loss0.42649770907213674\n",
            "iteration 848 :train_loss:0.4065214045885477 val_loss0.4264510086175685\n",
            "iteration 849 :train_loss:0.4064833281222964 val_loss0.4264038398317119\n",
            "iteration 850 :train_loss:0.40644535835036816 val_loss0.42635648720022973\n",
            "iteration 851 :train_loss:0.40640730239138184 val_loss0.4263098495687479\n",
            "iteration 852 :train_loss:0.406369088057795 val_loss0.4262628882546086\n",
            "iteration 853 :train_loss:0.40633053054804863 val_loss0.4262174719146684\n",
            "iteration 854 :train_loss:0.40629185405738566 val_loss0.4261718377118973\n",
            "iteration 855 :train_loss:0.40625317833701236 val_loss0.42612586893769366\n",
            "iteration 856 :train_loss:0.4062145145373186 val_loss0.4260811251892116\n",
            "iteration 857 :train_loss:0.406175992396553 val_loss0.42603580903247223\n",
            "iteration 858 :train_loss:0.406137680595341 val_loss0.4259891886072773\n",
            "iteration 859 :train_loss:0.40609950299684117 val_loss0.42594279704522864\n",
            "iteration 860 :train_loss:0.40606119684143654 val_loss0.42589669070640634\n",
            "iteration 861 :train_loss:0.40602285165707075 val_loss0.42585094920583405\n",
            "iteration 862 :train_loss:0.40598423884799434 val_loss0.42580633468444473\n",
            "iteration 863 :train_loss:0.40594554341535305 val_loss0.42576028475763933\n",
            "iteration 864 :train_loss:0.4059068039395241 val_loss0.4257143275003925\n",
            "iteration 865 :train_loss:0.4058681143571686 val_loss0.4256686535029884\n",
            "iteration 866 :train_loss:0.40582956891275507 val_loss0.42562372990519404\n",
            "iteration 867 :train_loss:0.405791061521758 val_loss0.42557839837552897\n",
            "iteration 868 :train_loss:0.40575237232717987 val_loss0.42553344938524634\n",
            "iteration 869 :train_loss:0.40571373426551743 val_loss0.4254900287177319\n",
            "iteration 870 :train_loss:0.4056748235530386 val_loss0.4254451868792237\n",
            "iteration 871 :train_loss:0.4056357546654477 val_loss0.42539989562079855\n",
            "iteration 872 :train_loss:0.4055963390247253 val_loss0.4253562422193607\n",
            "iteration 873 :train_loss:0.40555679669179584 val_loss0.4253093977609364\n",
            "iteration 874 :train_loss:0.405517331146653 val_loss0.42526310819934904\n",
            "iteration 875 :train_loss:0.40547789483252444 val_loss0.42521898051665624\n",
            "iteration 876 :train_loss:0.4054385467119054 val_loss0.42517207008648866\n",
            "iteration 877 :train_loss:0.4053992321528863 val_loss0.4251266402105247\n",
            "iteration 878 :train_loss:0.40535967771947373 val_loss0.42507911421267425\n",
            "iteration 879 :train_loss:0.4053201303442151 val_loss0.425032571676854\n",
            "iteration 880 :train_loss:0.40528073247278634 val_loss0.4249847249054085\n",
            "iteration 881 :train_loss:0.40524145409982215 val_loss0.42493691859902166\n",
            "iteration 882 :train_loss:0.40520224456634674 val_loss0.4248892021018598\n",
            "iteration 883 :train_loss:0.4051628926421299 val_loss0.42483992602326665\n",
            "iteration 884 :train_loss:0.4051234325241774 val_loss0.42479214228975565\n",
            "iteration 885 :train_loss:0.40508397695889803 val_loss0.42474271839435385\n",
            "iteration 886 :train_loss:0.40504444324398603 val_loss0.42469552006279987\n",
            "iteration 887 :train_loss:0.40500487096080684 val_loss0.42464924753034905\n",
            "iteration 888 :train_loss:0.40496534727838307 val_loss0.42460010898370715\n",
            "iteration 889 :train_loss:0.4049256198442898 val_loss0.4245528905115123\n",
            "iteration 890 :train_loss:0.40488576174105934 val_loss0.42450386377880167\n",
            "iteration 891 :train_loss:0.40484590174075463 val_loss0.4244567905979879\n",
            "iteration 892 :train_loss:0.40480610057834204 val_loss0.4244096813147654\n",
            "iteration 893 :train_loss:0.4047662635004147 val_loss0.42436052087888365\n",
            "iteration 894 :train_loss:0.40472632750654575 val_loss0.4243138640232014\n",
            "iteration 895 :train_loss:0.4046864291683279 val_loss0.4242652728507603\n",
            "iteration 896 :train_loss:0.40464641057931866 val_loss0.4242184073005882\n",
            "iteration 897 :train_loss:0.40460630390117686 val_loss0.424169396002755\n",
            "iteration 898 :train_loss:0.404566166549042 val_loss0.4241227411831811\n",
            "iteration 899 :train_loss:0.4045261182975379 val_loss0.42407635301715185\n",
            "iteration 900 :train_loss:0.40448600562044157 val_loss0.4240283862714374\n",
            "iteration 901 :train_loss:0.4044459116574541 val_loss0.42398297248099526\n",
            "iteration 902 :train_loss:0.40440577697041535 val_loss0.42393724513121156\n",
            "iteration 903 :train_loss:0.4043656401122642 val_loss0.42388864807621224\n",
            "iteration 904 :train_loss:0.4043255186165815 val_loss0.4238426334480906\n",
            "iteration 905 :train_loss:0.40428536164433804 val_loss0.42379441323784794\n",
            "iteration 906 :train_loss:0.40424521444306866 val_loss0.42374471891837573\n",
            "iteration 907 :train_loss:0.40420508370172037 val_loss0.4236976261809296\n",
            "iteration 908 :train_loss:0.40416493311324003 val_loss0.42364897439279303\n",
            "iteration 909 :train_loss:0.4041248811362644 val_loss0.42360055471917585\n",
            "iteration 910 :train_loss:0.4040850044740202 val_loss0.423553111465544\n",
            "iteration 911 :train_loss:0.4040452212423387 val_loss0.42350504799126754\n",
            "iteration 912 :train_loss:0.4040055335365958 val_loss0.4234573434204288\n",
            "iteration 913 :train_loss:0.4039658887963291 val_loss0.42341064910081544\n",
            "iteration 914 :train_loss:0.403926068652574 val_loss0.42336203569733266\n",
            "iteration 915 :train_loss:0.4038863528685556 val_loss0.4233119577385581\n",
            "iteration 916 :train_loss:0.4038465758030038 val_loss0.42326232152676546\n",
            "iteration 917 :train_loss:0.4038065926337248 val_loss0.4232133983630674\n",
            "iteration 918 :train_loss:0.4037663765364112 val_loss0.42316372954147563\n",
            "iteration 919 :train_loss:0.40372608713553554 val_loss0.423113888914258\n",
            "iteration 920 :train_loss:0.40368571931687036 val_loss0.42306389064701155\n",
            "iteration 921 :train_loss:0.4036453658794495 val_loss0.4230133173065481\n",
            "iteration 922 :train_loss:0.4036050410155617 val_loss0.42296324504528654\n",
            "iteration 923 :train_loss:0.4035646997511643 val_loss0.42291360531581385\n",
            "iteration 924 :train_loss:0.4035241381646555 val_loss0.4228641729761859\n",
            "iteration 925 :train_loss:0.4034836402879378 val_loss0.42281513483920097\n",
            "iteration 926 :train_loss:0.40344324628473793 val_loss0.4227670182274988\n",
            "iteration 927 :train_loss:0.40340263973059776 val_loss0.4227187433920938\n",
            "iteration 928 :train_loss:0.40336201709958364 val_loss0.4226706839781892\n",
            "iteration 929 :train_loss:0.40332124600385155 val_loss0.4226223097295926\n",
            "iteration 930 :train_loss:0.4032801919461965 val_loss0.4225736146118639\n",
            "iteration 931 :train_loss:0.40323912134240864 val_loss0.42252524530044755\n",
            "iteration 932 :train_loss:0.40319789297348796 val_loss0.42247714899498684\n",
            "iteration 933 :train_loss:0.40315637096390383 val_loss0.4224288726144412\n",
            "iteration 934 :train_loss:0.4031148084095568 val_loss0.42238051587027253\n",
            "iteration 935 :train_loss:0.40307319490411536 val_loss0.4223323601437232\n",
            "iteration 936 :train_loss:0.4030312895365217 val_loss0.4222838843134373\n",
            "iteration 937 :train_loss:0.4029891260535494 val_loss0.42223585578155065\n",
            "iteration 938 :train_loss:0.40294700289219687 val_loss0.4221883678106646\n",
            "iteration 939 :train_loss:0.4029048750172982 val_loss0.42214023401003964\n",
            "iteration 940 :train_loss:0.40286261698597836 val_loss0.42209239469142656\n",
            "iteration 941 :train_loss:0.4028202886668279 val_loss0.42204534388958387\n",
            "iteration 942 :train_loss:0.402777964439442 val_loss0.4219979481227215\n",
            "iteration 943 :train_loss:0.4027352271037651 val_loss0.4219504664620565\n",
            "iteration 944 :train_loss:0.40269265796171555 val_loss0.4219027024328002\n",
            "iteration 945 :train_loss:0.4026500635197425 val_loss0.42185563922502534\n",
            "iteration 946 :train_loss:0.40260743245628283 val_loss0.4218080686807876\n",
            "iteration 947 :train_loss:0.40256472202418797 val_loss0.42176030628754557\n",
            "iteration 948 :train_loss:0.4025216508949689 val_loss0.4217136581007639\n",
            "iteration 949 :train_loss:0.4024786028019321 val_loss0.4216658914212903\n",
            "iteration 950 :train_loss:0.40243566941041126 val_loss0.4216166926076067\n",
            "iteration 951 :train_loss:0.4023927147056096 val_loss0.42156757516162613\n",
            "iteration 952 :train_loss:0.40234962660705337 val_loss0.42151788093487963\n",
            "iteration 953 :train_loss:0.40230689963311084 val_loss0.4214682313428661\n",
            "iteration 954 :train_loss:0.4022642291207693 val_loss0.4214172683695893\n",
            "iteration 955 :train_loss:0.4022216895417231 val_loss0.42136614180181\n",
            "iteration 956 :train_loss:0.402179148271803 val_loss0.4213150137973701\n",
            "iteration 957 :train_loss:0.40213654791325426 val_loss0.4212639149780617\n",
            "iteration 958 :train_loss:0.40209408849855793 val_loss0.4212128216310012\n",
            "iteration 959 :train_loss:0.40205170159979164 val_loss0.42116295355667993\n",
            "iteration 960 :train_loss:0.4020091897559544 val_loss0.42111162425955595\n",
            "iteration 961 :train_loss:0.40196646446845957 val_loss0.42106271176408616\n",
            "iteration 962 :train_loss:0.4019236195561271 val_loss0.4210139250483685\n",
            "iteration 963 :train_loss:0.4018807080520371 val_loss0.4209649637733111\n",
            "iteration 964 :train_loss:0.40183769938951436 val_loss0.4209150998577574\n",
            "iteration 965 :train_loss:0.4017945099156949 val_loss0.4208660440242617\n",
            "iteration 966 :train_loss:0.4017506363698239 val_loss0.4208157502530751\n",
            "iteration 967 :train_loss:0.4017062043535702 val_loss0.4207651527920017\n",
            "iteration 968 :train_loss:0.4016618160672895 val_loss0.4207157322830333\n",
            "iteration 969 :train_loss:0.4016172589848302 val_loss0.4206663005102157\n",
            "iteration 970 :train_loss:0.4015726784860373 val_loss0.42061748884529504\n",
            "iteration 971 :train_loss:0.40152823473796906 val_loss0.42056788585271215\n",
            "iteration 972 :train_loss:0.40148378649705735 val_loss0.4205184438165895\n",
            "iteration 973 :train_loss:0.40143940399619965 val_loss0.4204687101685373\n",
            "iteration 974 :train_loss:0.40139511108662196 val_loss0.4204182397547178\n",
            "iteration 975 :train_loss:0.4013509837890432 val_loss0.420368264777909\n",
            "iteration 976 :train_loss:0.40130623739098487 val_loss0.42031770573648464\n",
            "iteration 977 :train_loss:0.4012614003717832 val_loss0.42026635457541184\n",
            "iteration 978 :train_loss:0.40121637696372325 val_loss0.4202155652695122\n",
            "iteration 979 :train_loss:0.40117135967748413 val_loss0.4201645280215415\n",
            "iteration 980 :train_loss:0.40112654653572294 val_loss0.42011389636012053\n",
            "iteration 981 :train_loss:0.401081761317778 val_loss0.4200636060641767\n",
            "iteration 982 :train_loss:0.40103688678108895 val_loss0.4200132891068566\n",
            "iteration 983 :train_loss:0.40099176915531193 val_loss0.41996268079271315\n",
            "iteration 984 :train_loss:0.4009463832801836 val_loss0.4199123103883748\n",
            "iteration 985 :train_loss:0.400900739092179 val_loss0.4198610532262896\n",
            "iteration 986 :train_loss:0.4008551315140812 val_loss0.419810339115943\n",
            "iteration 987 :train_loss:0.40080916161415936 val_loss0.4197555377706724\n",
            "iteration 988 :train_loss:0.4007630188113409 val_loss0.4197065568010559\n",
            "iteration 989 :train_loss:0.40071703148665483 val_loss0.41965315880969695\n",
            "iteration 990 :train_loss:0.40067089923908966 val_loss0.41960096587638296\n",
            "iteration 991 :train_loss:0.4006242134927158 val_loss0.41955142561028164\n",
            "iteration 992 :train_loss:0.4005774118218663 val_loss0.4194979223853952\n",
            "iteration 993 :train_loss:0.400530558997684 val_loss0.41945006389988104\n",
            "iteration 994 :train_loss:0.4004839343826648 val_loss0.4193948786926979\n",
            "iteration 995 :train_loss:0.40043730712536246 val_loss0.4193479836408218\n",
            "iteration 996 :train_loss:0.40039072072710813 val_loss0.41929649269361746\n",
            "iteration 997 :train_loss:0.4003439588012707 val_loss0.4192467307786526\n",
            "iteration 998 :train_loss:0.40029738658613334 val_loss0.4191954432388462\n",
            "iteration 999 :train_loss:0.40025099973020933 val_loss0.41914419931945596\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9AcOMFcQISAy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}